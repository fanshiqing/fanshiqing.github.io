<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Shiqing Fan&#39;s Blog</title>
  
  <subtitle>Hello, world!</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://fanshiqing.github.io/"/>
  <updated>2017-11-24T09:31:07.000Z</updated>
  <id>http://fanshiqing.github.io/</id>
  
  <author>
    <name>Shiqing Fan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>分布式一致性算法——Paxos简介</title>
    <link href="http://fanshiqing.github.io/2017/11/23/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94Paxos%E7%AE%80%E4%BB%8B/"/>
    <id>http://fanshiqing.github.io/2017/11/23/分布式一致性算法——Paxos简介/</id>
    <published>2017-11-23T08:31:03.000Z</published>
    <updated>2017-11-24T09:31:07.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>在分布式系统中，一个核心的问题就是数据一致性问题。Paxos一致性算法是分布式系统中的经典算法，由分布式大师Lamport提出，用来解决一个分布式系统中如何就某个决议（值）达成一致的问题，基于消息传递且具有高度容错性。用好理解的方式说，就是在一个选举过程中，让不同的选民最终做出一致的决定。Lamport本人也因其对分布式系统的杰出理论贡献获得了2013年的图灵奖。</p><a id="more"></a><blockquote><p><strong>Paxos</strong> is a mechanism for achieving consensus on a single value over unreliable communication channels.</p></blockquote><h2 id="Paxos算法分析"><a href="#Paxos算法分析" class="headerlink" title="Paxos算法分析"></a>Paxos算法分析</h2><h3 id="分布式一致性问题-The-distributed-consensus-problem）"><a href="#分布式一致性问题-The-distributed-consensus-problem）" class="headerlink" title="分布式一致性问题(The distributed consensus problem）"></a>分布式一致性问题(The distributed consensus problem）</h3><blockquote><p><strong>Consistency</strong><br>Get multiple servers to agree on state.</p><p><strong>Consistenty类型</strong></p><ul><li><strong>Strong</strong>: 强一致性。要求无论更新操作是在哪一个副本执行，之后所有的读操作都要能获得最新的数据（对用户最友好，但是性能影响大）;</li><li><strong>Weak</strong>:<br>弱一致性。用户读到某一操作对系统特定数据的更新需要一段时间，我们称这段时间为“不一致性窗口”。但经过“<em>不一致时间窗口</em>”这段时间后，后续对该数据的读取都是更新后的值 (best effort only, used for cache);</li><li><strong>Eventually</strong><br>最终一致性。是弱一致性的一种特例，保证用户最终能够读取到某操作对系统特定数据的更新(DNS是典型的最终一致性系统)。</li></ul></blockquote><p>在分布式系统中，我们如何能够在多个选项中选择一个单独的动作（action）呢？又该如何让这个动作能以容错（fault-tolerant）的方式来完成呢？这是分布式共识问题的基本问题。这个问题听起来似乎很简单：只需要选择一个节点来做决定嘛！但显然这会引入单点失效问题：如果这个唯一的决策者失效了，那所有其他节点就没有办法知道该采取哪个行动了（这其实违反了接下来要介绍的Paxos原则之一的存活性原则）。那在决策者失效时重新选择一个决策者会如何？但这可能会导致最终有两个决策者，而每个决策者可能做出不同的决定，导致共识不能达成。好吧这个问题似乎不那么简单。让我们来看看Paxos是如何解决这个问题的。</p><h3 id="两个原则（requirements）"><a href="#两个原则（requirements）" class="headerlink" title="两个原则（requirements）}"></a>两个原则（requirements）}</h3><p>现在我们已经对这个一致性问题有了直觉上的认识，下面来阐述解决方案的两个原则：安全原则和存活原则。</p><ul><li>安全原则 (safety)：系统不会产生“错误”的答案<ul><li>只有被提议（proposed）的值才可能被选中（chosen)</li><li>只有一个值被选中</li><li>一个节点永远不会知道一个值已经被选择，除非它本身已经被选中 （一旦达成共识，这个事实被公之于众，且不可更改(回退？)）</li></ul></li><li>存活原则 (liveness)：只要多数节点存活并且彼此间可以通信，最终都要做到的事情（正常运行）<ul><li>某个被提议的值最终将被选中</li><li>如果某个值已经被选中，那么其他节点最终可以学习到这个值</li></ul></li></ul><h3 id="基本假设（assumptions）"><a href="#基本假设（assumptions）" class="headerlink" title="基本假设（assumptions）"></a>基本假设（assumptions）</h3><p>除了上述的两条原则之外，我们还需要对分布式系统的工作原理以及可能失效的情形作出一些假设。分布式系统中的节点通信主要存在两种模型：共享内存和消息传递。Paxos算法基于消息传递。在基于消息传递的分布式系统中，不可避免地会出现消息延迟、丢失、重复的情形。此外，在消息传递模型中，节点故障可以分为失效停止（fail stop）和拜占庭错误（Byzantine Failure）（所有不是失效-停止的错误都可以归为拜占庭错误）两大类，基础Paxos算法不考虑拜占庭将军问题。更具体地，基础Paxos算法的基本假设具体如下。</p><ul><li><strong>失效-停止</strong> 假设<ul><li>失败-停止： 当一个节点发生故障时，它将完全停止工作</li><li>重启：故障的节点重新启动时可以恢复正常操作</li></ul></li><li><strong>消息</strong>（messages）相关的假设：基于消息传递的异步通信<ul><li>消息可以丢失</li><li>消息可以重复发送</li><li>消息可以延迟(只要等待足够的时间，消息就会被送到)（乱序到达）但是消息不会被篡改（即不考虑拜占庭错误）</li></ul></li><li><strong>稳定的存储</strong>：假定系统中的节点可以访问某种形式的稳定的存储器，这些存储器可以保存故障之前记录的信息，直到节点重启并从故障中恢复（为什么需要稳定的存储？因为每个节点以任意的速度异步操作，可能因为fail stop而重启。所以任何一个节点都可能会在提议被选择后停机再重启，因此解决方案要求节点必须要能够记忆某些信息，从而能在重启后重新载入）</li></ul><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>在《Paxos made simple》论文描述中将一致性算法中节点的角色分类3类：proposers, acceptors和learners。不同的角色对应于一致性算法中的不同职责。更具体的，算法描叙相关的基本概念如下：</p><ul><li>proposal value: 提议的值</li><li>proposal number: <strong>提议编号</strong></li><li>proposal: 提议（= 提议编号 + 提议值）</li><li>proposor: 提议发起者，提出不同的建议值（proposoal）供决策者考虑。<code>提议是&lt;N, V&gt;对</code></li><li>acceptor： 提议接收者，负责达成共识。（接受符合一定限制的提案）</li><li>learner：最终决策学习者，不参与提议的处理，学习提议的处理结果。</li></ul><p>实际实现中，一个节点往往同时扮演这多个角色（通常是3个）。当在某个提议值上达成共识时我们就说这个值被选出来了。简单起见，在下面的算法分析中，我们只考虑Basic Paxos算法（没有learner这个角色）。</p><h4 id="提议编号"><a href="#提议编号" class="headerlink" title="提议编号"></a>提议编号</h4><p>Paxos能在不可靠的通信信道上进行操作的关键之一是为每个proposal分配一个唯一的ID。Proposal ID是什么以及为什么重要的细节将在稍后带来，这里我们先将这些ID视为与<strong>时间戳（timestamps）</strong>类似的角色：它们不仅让peers能从逻辑上判断一条消息是否比另一条消息更新，而且绕开了分布式系统中一致的、共享的时间概念（notions of time）所固有的复杂性。</p><p>Proposal ID的常见实现是一个简单的整数和一个用于确保唯一性的一个“tie-breaker”组成数对。这里使用的ID的pair的组合模式是 &lt; Integer, 节点的ID（具有唯一性）&gt;， 例如，（1，A）。这些ID是可比较的。为了确定哪个ID在逻辑上更新，第一个整数首先以正常的方式进行比较，并且如果它们相等，则将这些唯一的ID字符串进行字典序比较以打破不分胜负的局面，因而有（5，B）&gt;（4，B）&gt;（4，A）。</p><p><strong>ID的主要目的是保护算法免受延迟和/或重复的消息的影</strong>响。 Paxos使用一个非常简单的规则来实现这一点。如果新收到的消息的ID大于上一个处理的消息的ID，则对其进行处理。否则忽略这条消息。忽略延迟信息的动机并不像忽略重复消息那样显而易见，其原因来自于在Paxos这样一个多步骤的算法中, 与旧的和过期的step相关的消息不被允许干扰当前active step的消息。</p><h3 id="Paxos算法流程"><a href="#Paxos算法流程" class="headerlink" title="Paxos算法流程"></a>Paxos算法流程</h3><p>Paxos算法具体分为两个阶段。</p><ul><li>第一阶段（<strong>prepare</strong>）： 提议者选择一个全局唯一的建议值n，并向多数的（过半）接受者（majority of acceptors）发送一个准备请求（prepare request）：prepare(n)。如果一个接受者接收到一个值为n的准备请求，且该请求值比它看到的任何请求值都大，那么它对该请求作出确认答复promise(n)，承诺不再接收任何小于这个n的提议（<strong>并且包括它已经接收了的编号最高的提案（如果有的话）</strong>）</li><li>第二阶段（<strong>accept</strong>）： 如果proposer收到过半的acceptor的确认消息表示他们不会接受n以下的提议, 那么这个proposer给这些acceptor返回附带他的建议值v的确认消息（Accept(n, v)）. 如果一个acceptor收到这样的确认消息{n, v}, acceptor就会接受它(Accepted(n, v)). （<strong>除非在此期间acceptor又收到了一个比n更大的提议.</strong>）</li></ul><p>Basic Paxos算法流程具体流程如下图：</p><p><img src="http://static.zybuluo.com/sqfan/8ilewyjhouw644hjqk9j35og/image_1bvmajadnc8nsuh1tqe1ved19kd25.png" alt="image_1bvmajadnc8nsuh1tqe1ved19kd25.png-402.8kB"></p><p>这里还需要对一个提议值被选择的概念做一个阐述：一个提议值n被选择当且仅当在上述算法的第二阶段超过半数的接收者接收了该值。这里有必要对Paxos算法里的这个majority这个概念多加阐述。</p><blockquote><p>强大多数（Strong Majority/Quorum): 一组由一半以上的接受者组成的集合。</p></blockquote><p>通过上面的描述可知，在节点间达成共识的关键思想就是这个“强大多数”的概念。这个概念的特殊之处在于：任意两个“强大多数”的集合都至少有一个共同的成员节点。Paxos算法通过让提议者向“强大多数”的接收者发送提议来利用这个特性：如果我们可以得到超过半数的节点来对某个单一的提议值达成共识，那么我们已经达成一致；因为所有其他Quorum将包含至少一个此Quorum的节点，而交集内的进程就可以保证正确性被继承，以后被传播出去。同时，这也意味着我们一次可以容忍的错误节点的数量： 若正常工作的节点小于等于半数个节点，就不可能形成多数。</p><p>那为什么要分为两个阶段呢？先说一下prepare阶段的作用。第一，检查是有已经有被批准的值，如果有的话，就用被批准的值。第二，如果之前存在提议还没有被接受，则干掉他们（以便不让他们和我们发生竞争）。</p><p>第一阶段中proposal ID (即prepare()发送的n)有什么特性呢？Paxos算法要求Proposal ID全局唯一且递增。为什么要这样设计呢？这个ID实际上相当于一个虚拟时钟（virtual clock), 所有的acceptors都会使用这个ID，用来保证当它收到任意节点发来的消息时，该消息是最新的（或者说是目前处于Paxos算法最新一轮的消息）而不是过时的消息（系统异步环境下消息的允许任意延迟）。事实上，全局唯一ID（且递增）的生成本身是需要一定的技术来保证的：毕竟需要同时满足高并发、高可用、低延迟这样的需求可不简单。微信的seqvr系统就是专门用来做这种序列生成的。</p><p>是不是算法已经完全没有问题了呢？根据上述过程，当一个proposer发现存在编号更大的提案时将终止提案。这意味着提出一个编号更大的提案会终止之前的提案过程。如果两个节点在这种情况下都转而提出一个编号更大的提案，就可能陷入活锁（live lock），违背了Progress的原则。这种情况下的解决方案是选举出一个leader（用我们之前学的leader election知识），仅允许leader提出提案。但是由于消息传递的不确定性（异步通信模型），可能有多个proposer自认为自己已经成为leader（也就是没有达成谁是leader的consensus）。</p><h3 id="一个示例（不考虑failures）"><a href="#一个示例（不考虑failures）" class="headerlink" title="一个示例（不考虑failures）"></a>一个示例（不考虑failures）</h3><p>2个proposor, 3个acceptor, 1个learner.</p><ul><li>Phase 1: Prepare/Promise </li></ul><p><img src="http://static.zybuluo.com/sqfan/zqieqklcxmqscwrcv5504jks/image_1bvmj4ee41pfg1ech1pvtlhh1bn12q.png" alt="image_1bvmj4ee41pfg1ech1pvtlhh1bn12q.png-82.9kB"></p><p>Proposor A和Proposal B分别向3个acceptors发送准备请求prepare(2)和prepare(4).</p><p><img src="http://static.zybuluo.com/sqfan/bazc3scx2b8uh775pfx1ng4k/image_1bvmj3o9go1a4f61bq9j2ad052d.png" alt="image_1bvmj3o9go1a4f61bq9j2ad052d.png-108kB"></p><p>Prepare(2)率先到达Acceptor X,Y. 这两个acceptor向A作出确认答复Promise(0,null)。<br>Prepare(4)率先到达Acceptor Z. Z向B作出确认答复Promise(0,null)。</p><p><img src="http://static.zybuluo.com/sqfan/gd8aghb5o63zx8yijyqcv4go/image_1bvmj2o589801qd21nof81i1c20.png" alt="image_1bvmj2o589801qd21nof81i1c20.png-126.8kB"></p><p>Proposor B的Prepare(4)之后于A到达Acceptor X,Y. X,Y检查发现该请求值（4）比它看到的任何请求值都大（最大是之前A发来的prepare(2)）,那么它对该请求作出确认答复promise(2,null)。</p><p>Proposor A的Prepare(2)之后于B到达Acceptor Z.因为请求值（2）小于之前处理的最大请求值4，该消息被忽略。</p><ul><li>Phase 2: Accept/Accepted<br><img src="http://static.zybuluo.com/sqfan/rt9lujh3o79azyjlln6vz782/image_1bvmj64o24gqa4m1fut1qgv13k837.png" alt="image_1bvmj64o24gqa4m1fut1qgv13k837.png-78.7kB"></li></ul><p>Proposal A收到了2/3(超过半数)的promise消息，于是给这些acceptor返回附带他的建议值v的确认消息<code>Accept(2,100)</code>. 因为在phase1之后所有的acceptor都看到了目前最大的promise值是B发来的4，所以A本次发给acceptors的accept消息都将被忽略。</p><p><img src="http://static.zybuluo.com/sqfan/4ilw569xoojo21mlbbzxms2h/image_1bvmj6lggiit1oik14nq20u1maj3k.png" alt="image_1bvmj6lggiit1oik14nq20u1maj3k.png-99.1kB"><br>Proposal B也收到了3/3(超过半数)的promise消息，于是给这些acceptor返回附带他的建议值v的确认消息<code>Accept(4,50)</code>. Acceptor X，Y, Z检查本地最大提议值和Accept的ID相同，于是将<4,50>写入本地，并返回Accepted（）消息表示确认接受。Learner会学习到这一情况并把value=50被选择的情况记录下来，并告知proposer A.</4,50></p><h3 id="Paxos的属性"><a href="#Paxos的属性" class="headerlink" title="Paxos的属性"></a>Paxos的属性</h3><p>Paxos的属性论文里有详细列出，为了完整起见，我还是列在这里，完整的推导过程可以参见维基百科上的<a href="https://zh.wikipedia.org/wiki/Paxos算法" target="_blank" rel="external">Paxos算法</a>。</p><ul><li>P1：一个acceptor必须接受（accept）第一次收到的提案。</li><li>P2：一旦一个具有value v的提案被批准（chosen），那么之后批准（chosen）的提案必须具有value v。<ul><li>P2a：一旦一个具有value v的提案被批准（chosen），那么之后任何acceptor再次接受（accept）的提案必须具有value v。</li><li>P2b：一旦一个具有value v的提案被批准（chosen），那么以后任何proposer提出的提案必须具有value v。</li><li>P2c：如果一个编号为n的提案具有value v，那么存在一个多数派，要么他们中所有人都没有接受（accept）编号小于n<br>的任何提案，要么他们已经接受（accept）的所有编号小于n的提案中编号最大的那个提案具有value v。</li></ul></li></ul><h3 id="Paxos算法主要贡献"><a href="#Paxos算法主要贡献" class="headerlink" title="Paxos算法主要贡献"></a>Paxos算法主要贡献</h3><p>Paxos算法是第一个被证明的一致性算法，也是最著名的分布式一致性算法。Google的粗粒度锁服务<code>Chubby</code>的底层一致性实现就是以Paxos算法为基础。用户只需调用Chubby的锁服务接口就能够实现分布式系统中多个进程之间粗粒度的同步控制，从而保证分布式数据的一致性。Chubby的开发者对Paxos曾有过这样的评价：“所有的一致性协议本质上要么是Paxos，要么就是其变体”。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>Paxos算法可以说是分布式系统中最重要的一个算法。虽然Lamport在《Paxos made simple》中声称“Paxos is among the simplest and most obvious of distributed algorithms”, 但是个人在仔细研读几遍这篇论文后，还是觉得文章还是比较晦涩难懂的，虽然全文没有涉及到一个数学公式，都是文字描述，但是也没有直观的图来说明，最后还是查阅了其他各路资料，其中Chris Colohan在油管上一个关于paxos的lectture帮助很大。感觉Paxos最难理解的地方就在于要能搞清楚是什么因素导致该协议以这种方式呈现，而非其最终协议本身的内容。结合具体的应用场景来理解Paxos或许更好一点。</p><p>在本篇讨论中，实际上是讨论了Basic Paxos算法。而Basic Paxos只是理论模型，实际中使用的性能上更好的Multi-Paxos。Multi Paxos基于Basic Paxos, 将原来的2阶段过程（至少需要2次网络交互 + 2次本地持久化）简化为1个阶段，从而加速了提交速度。但是由于Multi-Paxos在理解和实现的复杂性，另一个改进版的一致性协议——Raft应运而生。Raft相比Paxos而言更加健全，更加易懂，虽然其在核心协议上基本是继承了Paxos中基于多数派的协议，但是其核心的贡献在于定义了易于实现的分布式一致性协议的事实标准。从某种意义上说，Raft不能算是一个新的协议，而是Paxos的一个具体化和简单化版本。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] Lamport, Leslie. “Paxos made simple.” ACM Sigact News 32.4 (2001): 18-25.<br>[2] <a href="https://www.youtube.com/watch?v=VdSO8g1Yv0w" target="_blank" rel="external">Paxos lecture by <strong>Chris Colohan</strong>, video</a><br>[3] <a href="https://understandingpaxos.wordpress.com/" target="_blank" rel="external">Understanding Paxos, blog</a><br>[4] <a href="http://people.csail.mit.edu/alinush/6.824-spring-2015/stumbled/paxos-explained-from-scratch.pdf" target="_blank" rel="external">Tutorial Summary: Paxos Explained from Scratch, pdf</a><br>[5] <a href="https://courses.cs.washington.edu/courses/cse452/13sp/resources/cse452_sec3.pdf" target="_blank" rel="external">Paxos by example, ppt</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景介绍&quot;&gt;&lt;a href=&quot;#背景介绍&quot; class=&quot;headerlink&quot; title=&quot;背景介绍&quot;&gt;&lt;/a&gt;背景介绍&lt;/h2&gt;&lt;p&gt;在分布式系统中，一个核心的问题就是数据一致性问题。Paxos一致性算法是分布式系统中的经典算法，由分布式大师Lamport提出，用来解决一个分布式系统中如何就某个决议（值）达成一致的问题，基于消息传递且具有高度容错性。用好理解的方式说，就是在一个选举过程中，让不同的选民最终做出一致的决定。Lamport本人也因其对分布式系统的杰出理论贡献获得了2013年的图灵奖。&lt;/p&gt;
    
    </summary>
    
      <category term="Introduction to Distributed Algorithms" scheme="http://fanshiqing.github.io/categories/Introduction-to-Distributed-Algorithms/"/>
    
    
      <category term="分布式算法" scheme="http://fanshiqing.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>L05-Fault-tolerant consensus</title>
    <link href="http://fanshiqing.github.io/2017/10/26/L05-Fault-tolerant-consensus/"/>
    <id>http://fanshiqing.github.io/2017/10/26/L05-Fault-tolerant-consensus/</id>
    <published>2017-10-26T08:31:03.000Z</published>
    <updated>2017-11-21T09:14:56.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h2><ul><li>The consensus problem</li><li>Consensus with Byzantine Failures</li></ul><a id="more"></a><h2 id="Processor-Failures-in-Message-Passing"><a href="#Processor-Failures-in-Message-Passing" class="headerlink" title="Processor Failures in Message Passing"></a>Processor Failures in Message Passing</h2><ul><li><code>Fail stop</code>: at some point the processor stops taking steps<ul><li>at the processor’s final step, it might succeed in sending only <strong>a subset of the messages</strong> it is supposed to send, e.g.: <ul><li><code>Crash</code></li><li>Power outage</li><li>Hardware failure</li><li>Out of memory/disk full</li></ul></li><li>Strategies:<ul><li>Checkpoint state and restart (<strong>High latency</strong>)</li><li>Replicate state and fail over (失效备援) (<strong>High cost</strong>)</li></ul></li></ul></li><li><code>Byzantine</code>: <strong>Everything that is not fail stop</strong>. Processor changes state arbitrarily and sends message with arbitrary content. <ul><li>E.g.:<ul><li>Bit flip in memory or on disk corrupts data</li><li>Older version of code on one node sends (now) invalid messages</li><li>Node starts running malicious version of software</li></ul></li><li>Goal: turn into fail stop<ul><li>Checksums/ECC (Error Correction Code)</li><li>Assertions</li><li>Timeouts</li></ul></li></ul></li></ul><h3 id="Failure-Matrix"><a href="#Failure-Matrix" class="headerlink" title="Failure Matrix"></a>Failure Matrix</h3><p><img src="http://static.zybuluo.com/sqfan/a3w5713ss4n0p18hy8q194g4/image_1bvc1f0gs1721tm5m0h17fq170j9.png" alt="image_1bvc1f0gs1721tm5m0h17fq170j9.png-44.5kB"></p><h3 id="Negative-result-for-link-failures"><a href="#Negative-result-for-link-failures" class="headerlink" title="Negative result for link failures"></a>Negative result for <strong>link failures</strong></h3><p>It is impossible to reach consensus in case of link failures, even in the synchronous case, and even if one only wants to tolerate a single link failure.</p><p>Consensus under link failures: <code>the 2 generals problem</code>.</p><ul><li>There are two generals of the same army who have encamped a short distance apart.</li><li>Their objective is to capture a hill, which is possible only if they attack simultaneously. </li><li>If only one general attacks, he will be defeated. </li><li>The two generals can only communicate by sending messengers, <strong>which is not reliable</strong>. </li><li>Is it possible for them to attack simultaneously?</li></ul><p><img src="http://static.zybuluo.com/sqfan/y946xgli1f5gy525hehs9fft/image_1bves2gnp6fhd301i32eni1dos83.png" alt="image_1bves2gnp6fhd301i32eni1dos83.png-39.6kB"></p><p>See <a href="https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf" target="_blank" rel="external">Fischer/Lynch/Paterson</a> for more discussion.</p><h2 id="The-Consensus-Problem"><a href="#The-Consensus-Problem" class="headerlink" title="The Consensus Problem"></a>The Consensus Problem</h2><p>Each process starts with an individual <code>input</code> from a particular value set V. Processes may fail by crashing. All non-faulty processes are required to produce outputs from the value set V, subject to simple agreement and validity. A solution to the <strong>consensus proble</strong>m must guarantee the following:</p><ul><li><code>Termination</code>: Eventually every <strong>nonfaulty processor</strong> must decide on a value (decision is irrevocable).</li><li><code>Agreement</code>: All decisions by nonfaulty processors must be the same.</li><li><code>Validity</code>: If all inputs are the same, then the decision of a nonfaulty processor must equal the common input</li></ul><p>Once a processor crashes, it is of no interest to the algorithm, and no requirements are placed on its decision.</p><p><strong>f-resilient system</strong>:</p><ul><li>at most <code>f</code> processors may fail</li><li>The set of faulty processors may be <strong>different</strong> in different executions.</li></ul><p>In the last round:</p><ul><li><code>Clean crash</code>: none or all of the outgoing messages are sent</li><li><code>Not-clean crash</code>: an arbitrary set of its outgoing messages are delivered.</li></ul><h3 id="Consensus-algorithm-in-the-presence-of-crash-failures"><a href="#Consensus-algorithm-in-the-presence-of-crash-failures" class="headerlink" title="Consensus algorithm in the presence of crash failures"></a>Consensus algorithm in the presence of crash failures</h3><p>Each process maintains a set of the values it knows to exist in the system; initially, this set contains only its own input. At the first round, each process broadcasts its own input to all processes. For the subsequent f rounds, each process takes the following actions:</p><ul><li>updates its set by joining it with the sets received from other<br>processes, and</li><li>broadcasts any new additions to the set to all processes.</li></ul><p>After f+1 rounds, the process decides on the smallest value in its set.</p><p><img src="http://static.zybuluo.com/sqfan/lgf5n4i2dja5bmzism90iizr/image_1bveikf7ge2e1bc612bv1vie1e51g.png" alt="image_1bveikf7ge2e1bc612bv1vie1e51g.png-69.6kB"></p><p><img src="http://static.zybuluo.com/sqfan/xn1sspjaamzg1dl37xu5z5mm/image_1bvelori51oup18o6ddh1978mjk1t.png" alt="image_1bvelori51oup18o6ddh1978mjk1t.png-65.1kB"></p><ul><li>Intuition for <code>Agreement</code>: Assume that a process pi decides on a value x smaller than that decided by some other process p_j. Then, x has remained “hidden” from pj for (f+1) rounds. We have at most f faulty processes. A contradiction!!!</li><li>Number of processes: n &gt; f</li><li>Round Complexity: f + 1</li><li><strong>Message Complexity</strong>: (at most) n^2 * |V| messages, where V is the set of input values.</li></ul><h4 id="Worst-case-scenario"><a href="#Worst-case-scenario" class="headerlink" title="Worst case scenario"></a>Worst case scenario</h4><p><img src="http://static.zybuluo.com/sqfan/l5okgj5uwhlx4c9qcpuwb7jt/f-algorithm.gif" alt="f-algorithm.gif-145.7kB"></p><h2 id="Consensus-with-Byzantine-Failures"><a href="#Consensus-with-Byzantine-Failures" class="headerlink" title="Consensus with Byzantine Failures"></a>Consensus with Byzantine Failures</h2><blockquote><p><strong>Theorem (5.7)</strong>:  Any consensus algorithm for 1 Byzantine failure must have at least 4 processors.</p></blockquote><p>Proof of Theorem 5.7:<br><img src="http://static.zybuluo.com/sqfan/fggijnartoo9il036cnmmsee/image_1bven6g7d173d11ef12j8kvdiin34.png" alt="image_1bven6g7d173d11ef12j8kvdiin34.png-61.5kB"></p><blockquote><p><strong>Theorem</strong>:  Any consensus algorithm for <em>f</em> Byzantine failures must have at least <em>3f+1</em> processors.</p></blockquote><ul><li>Partition the processors into three sets P0, P1, P2;<ul><li>Each containing at most n/3 processors</li></ul></li><li>P0 simulates p0, P1 simulates p1 and P2 simulates p2</li><li>n processors solves consensus =&gt; {p0, p1, p2} solves consensus.  ==&gt; Contradiction.</li></ul><h2 id="Exponential-Information-Gathering-EIG-Algorithm"><a href="#Exponential-Information-Gathering-EIG-Algorithm" class="headerlink" title="Exponential Information Gathering (EIG) Algorithm"></a>Exponential Information Gathering (EIG) Algorithm</h2><p>This algorithm uses</p><ul><li>f + 1 rounds (optimal)</li><li>n = 3f + 1 processors (optimal)</li><li>exponential size messages (sub-optimal)</li></ul><p>Each processor keeps a tree data structure in its local state. Values are filled in the tree during the f + 1 rounds<br>At the end, the values in the tree are used to calculate the decision.</p><h3 id="Local-Tree-Data-Structure"><a href="#Local-Tree-Data-Structure" class="headerlink" title="Local Tree Data Structure"></a>Local Tree Data Structure</h3><ul><li>Each tree node is labeled with <strong>a sequence of</strong> unique processor indices.</li><li>Root’s label is empty sequence ; root has level 0</li><li>Root has <code>n</code> children, labeled 0 through n - 1</li><li>Child node labeled i has n - 1 children, labeled i : 0 through i : n-1 (<strong>skipping i : i</strong>)</li><li>Node at level <code>d</code> labeled <code>v</code> has <code>n - d</code> children, labeled v : 0 through v : n-1 (<strong>skipping any index appearing in v</strong>)</li><li>Nodes at level f + 1 are leaves.</li></ul><p>The tree when n = 4 and f = 1 :</p><p><img src="http://static.zybuluo.com/sqfan/joxki8ngdk011adx9codwgn7/image_1bveqt9ttg1pfv5qjv13h615cv7m.png" alt="image_1bveqt9ttg1pfv5qjv13h615cv7m.png-48.1kB"></p><h3 id="Filling-in-the-Tree-Nodes"><a href="#Filling-in-the-Tree-Nodes" class="headerlink" title="Filling in the Tree Nodes"></a>Filling in the Tree Nodes</h3><ul><li>Initially store your input in the root (level 0)</li><li>Round 1:  <ul><li>send level 0 of your tree to all</li><li>store value x received from each p_j in tree node labeled j (level 1); use a default if necessary</li><li>“pj told me that pj ‘s input was x”</li></ul></li><li>Round 2:<ul><li>send level 1 of your tree to all</li><li>store value x received from each pj for each tree node k in tree node labeled k : j (level 2); use a default if necessary</li><li>“pj told me that pk told pj that pk’s input was x”</li></ul></li><li>Continue for f + 1 rounds</li></ul><h3 id="Calculating-the-Decision"><a href="#Calculating-the-Decision" class="headerlink" title="Calculating the Decision"></a>Calculating the Decision</h3><ul><li>In round f + 1, each processor uses the values in its tree to compute its decision.</li><li><p>Recursively compute the “resolved” value for the root of the tree, <code>resolve()</code>, based on the “resolved” values for the other tree nodes:</p><p>  <img src="http://static.zybuluo.com/sqfan/hw0m0wv2ji8j69uweawp48pm/image_1bvepphnj1olttd012914ka1m8662.png" alt="image_1bvepphnj1olttd012914ka1m8662.png-14.8kB"></p></li></ul><p>Example of resolving values when n = 4 and f = 1:</p><p><img src="http://static.zybuluo.com/sqfan/6jwenw1oshgpwdg69okcj6qz/image_1bvepthlf1ipjgsuuea1qej14ko6f.png" alt="image_1bvepthlf1ipjgsuuea1qej14ko6f.png-34.9kB"></p><h2 id="A-Polynomial-Algorithm-The-King-Algorithm"><a href="#A-Polynomial-Algorithm-The-King-Algorithm" class="headerlink" title="A Polynomial Algorithm (The King Algorithm)"></a>A Polynomial Algorithm (The King Algorithm)</h2><p>We can reduce the message size to polynomial with a simple algorithm.</p><ul><li>The number of processors increases to: n &gt; 4f</li><li>The number of rounds increases to 2(f + 1)</li><li>Uses f+1 phases, each taking two rounds.</li></ul><p><img src="http://static.zybuluo.com/sqfan/imhi1feobwcw0fvkt1i1brbb/image_1bveqb3lk1gdip7m1mnu1a791j6a79.png" alt="image_1bveqb3lk1gdip7m1mnu1a791j6a79.png-106kB"></p><h2 id="Asynchronous-Consensus"><a href="#Asynchronous-Consensus" class="headerlink" title="Asynchronous Consensus"></a>Asynchronous Consensus</h2><p>Assumptions</p><ul><li>Communication system is reliable</li><li>Only processor failures (crash / Byzantine)</li><li>Completely asynchronous</li></ul><blockquote><p><strong>Theorem</strong>: For n ≥ 2, there is no algorithm in the read/write shared memory model that solves the agreement problem and guarantees wait-free termination</p></blockquote><p><code>Consensus is impossible</code>!!!</p><ul><li>Even in the presence of one single<br>processor (crash) failure</li></ul><p>Proof of impossibility</p><ul><li>Impossibility for shared memory<ul><li>The (n-1)-resilient case (wait-free case)</li><li>The 1-resilient case</li></ul></li><li>Impossibility for message passing <ul><li>Simulation</li></ul></li><li>Impossibility motivates the use of <code>failure detectors</code>.<ul><li>E.g., “The weakest failure detector for solving consensus”, JACM 43(4).</li></ul></li></ul><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Let f be the maximum number of faulty processors.</p><p><img src="http://static.zybuluo.com/sqfan/19nqclsounkyu7x122nfsvow/image_1bvesf5cr18v9imp1b2l19cbkp2aq.png" alt="image_1bvesf5cr18v9imp1b2l19cbkp2aq.png-45.2kB"></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] <a href="http://hagit.net.technion.ac.il/publications/dc/" target="_blank" rel="external">Attiya, Hagit, and Jennifer Welch. <strong>Distributed computing</strong>: fundamentals, simulations, and advanced topics. Vol. 19. John Wiley &amp; Sons, 2004.</a>.<br>[2] <a href="https://www.youtube.com/watch?v=_e4wNoTV3Gw&amp;index=6&amp;list=PLOE1GTZ5ouRPbpTnrZ3Wqjamfwn_Q5Y9A" target="_blank" rel="external">Byzantine Fault Tolerance, Distributed System (A free online class), by Chris Colohan</a>.<br>[3] The Byzantine Generals Problem, Leslie Lamport, Robert Shostack and Mashall Peace.  ACM TOPLAS 4.3, 1982.<br>[4] <a href="https://parasol.tamu.edu/~welch/teaching/668.s14/" target="_blank" rel="external">CSCE 668: Distributed Algorithms and Systems Spring 2014</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Preface&quot;&gt;&lt;a href=&quot;#Preface&quot; class=&quot;headerlink&quot; title=&quot;Preface&quot;&gt;&lt;/a&gt;Preface&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;The consensus problem&lt;/li&gt;
&lt;li&gt;Consensus with Byzantine Failures&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Introduction to Distributed Algorithms" scheme="http://fanshiqing.github.io/categories/Introduction-to-Distributed-Algorithms/"/>
    
    
      <category term="分布式算法" scheme="http://fanshiqing.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>L04-Mutual Exclusion</title>
    <link href="http://fanshiqing.github.io/2017/10/19/L04-Mutual-Exclusion/"/>
    <id>http://fanshiqing.github.io/2017/10/19/L04-Mutual-Exclusion/</id>
    <published>2017-10-19T06:35:55.000Z</published>
    <updated>2017-11-15T08:48:05.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h2><ul><li>The MUTEX problem<ul><li>the shared memory model</li><li>problem definition</li></ul></li><li>The unbounded algorithm<ul><li>The <strong>bakery algorithm</strong></li></ul></li><li>The unbounded algorithm</li></ul><a id="more"></a><h2 id="Shared-Memory-Model"><a href="#Shared-Memory-Model" class="headerlink" title="Shared Memory Model"></a>Shared Memory Model</h2><p>Processes communicate via a set of <strong>shared variables</strong> (also called <strong>shared registers</strong>), each shared variable has a <strong>type</strong>, defining a set of primitive operations (performed <code>atomically</code>)</p><p><img src="http://static.zybuluo.com/sqfan/wn08syzyz5d39z9hsf6n9f9w/image_1busp6ur1kh31rf41r7t12kl1kd66h.png" alt="image_1busp6ur1kh31rf41r7t12kl1kd66h.png-17.6kB"></p><p>Several types of shared variable can be employed, e.g.</p><ul><li>read/write</li><li>read-modify-write (RMW)</li><li>compare&amp;swap (CAS)</li></ul><p>Each register has a type, which specifies:</p><ul><li>Values to be taken on by registers</li><li>Operations performed on the registers</li><li>Values to be returned by operations (if any)</li><li>New values of the register resulting from the operation</li></ul><p>A <code>configuration</code> in the shared memory model is a vector: C = </p><p><img src="http://static.zybuluo.com/sqfan/qx9eqklt61x6x8p4nh5wzxyn/image_1buspq3steuh1t3n1r031h7o1ufb6u.png" alt="image_1buspq3steuh1t3n1r031h7o1ufb6u.png-14kB"></p><p>where q_i is a state of p_i and r_j is a value of register R.</p><p>The <code>events</code> in a shared memory system are:</p><ul><li>computation steps taken by the processors and are denoted by the index of the processor;</li><li>At each computation step, the shared variable is accessed.</li></ul><p>In asynchronous shared memory systems, an execution is <code>admissible</code> if each processor has an infinite number of computation steps.</p><h3 id="Complexity-measures"><a href="#Complexity-measures" class="headerlink" title="Complexity measures"></a>Complexity measures</h3><p>Obviously in shared memory systems there are no messages to measure. Instead we focus on the <code>space complexity</code>, the amount of shared memory needed to solve problems.</p><ul><li>Number of distinct shared variables required</li><li>and the amount of shared space (e.g., # of bits)</li></ul><h3 id="Changes-from-the-MSG-model"><a href="#Changes-from-the-MSG-model" class="headerlink" title="Changes from the MSG model"></a>Changes from the MSG model</h3><ul><li>Communication medium changes<ul><li>No inbuf and outbuf state components</li><li>Configuration includes values for shared variables</li></ul></li><li>Execution manner changes<ul><li>One event type: one computation step by a process<ul><li>pi’s state in old configuration specifies which shared variable is to be accessed and with which primitive</li><li>shared variable’s value in the new configuration changes according to the primitive’s semantics</li><li>pi’s state in the new configuration changes according to its old state and the result of the primitive</li></ul></li></ul></li></ul><h2 id="The-Mutual-Exclusion-Problem"><a href="#The-Mutual-Exclusion-Problem" class="headerlink" title="The Mutual Exclusion Problem"></a>The Mutual Exclusion Problem</h2><p>Each processor’s code is divided into four sections:</p><p><img src="http://static.zybuluo.com/sqfan/9a3ylq2nc360x9qn90v2uj6c/image_1buvcdt98c3u18sea2qpgp1funda.png" alt="image_1buvcdt98c3u18sea2qpgp1funda.png-13.1kB"></p><ul><li><code>Entry (trying)</code>: the code executed in preparation for entering the critical section</li><li><code>Critical</code>: the code to be protected from concurrent execution</li><li><code>Exit</code> (release): the code executed on leaving the critical section</li><li><code>Remainder</code>: the rest of the code</li></ul><p>Each processor cycles through these sections in the order: remainder –&gt; entry –&gt; critical –&gt; exit –&gt; remainder.</p><p>An algorithm for a shared memory system solves the <code>mutual exclusion</code> problem with no deadlock (or no lockout) if the following holds (three properties):</p><ul><li><code>Mutual exclusion</code>: In every configuration of every execution, at most one processor is in the critical section.</li><li><code>No deadlock</code>: In every admissible execution, if some processor is in the <strong>entry section</strong> in a configuration, then there is a later configuration in which <strong>some</strong> processor is in the critical section.</li><li><code>No lockout</code>: In every admissible execution, if some processor is in the <strong>entry section</strong> in a configuration, then there is a later configuration in which <strong>that same</strong> processor is in the critical section.</li></ul><p>Mutex progress conditions:</p><ul><li>no deadlock</li><li>no lockout</li><li><code>bounded waiting</code>: no lockout + <strong>while a processor is in its entry section, other processors enter the critical section no more than a certain number of times</strong>.</li></ul><p><strong>These three conditions are increasingly strong.</strong></p><p>The code for the entry and exit sections is allowed to assume that:</p><ul><li>no processor stays in its critical section forever</li><li>shared variables used in the entry and exit sections are not accessed during the critical and remainder sections</li></ul><h2 id="Mutual-Exclusion-Using-Powerful-Primitives"><a href="#Mutual-Exclusion-Using-Powerful-Primitives" class="headerlink" title="Mutual Exclusion Using Powerful Primitives"></a>Mutual Exclusion Using Powerful Primitives</h2><p>We will show that</p><ul><li><strong>one bit</strong> suffices for guaranteeing mutual exclusion with no deadlock</li><li>while <strong>O(logn)</strong> bits are necessary (and sufficient) for providing stronger fairness properties.</li></ul><h3 id="Binary-Test-amp-Set-Registers"><a href="#Binary-Test-amp-Set-Registers" class="headerlink" title="Binary Test&amp;Set  Registers"></a>Binary Test&amp;Set  Registers</h3><p>The <code>test&amp;set</code> operation <strong>atomically</strong> reads and updates the variable.</p><p><img src="http://static.zybuluo.com/sqfan/picjprmbz1mbyyz208r4p5x5/image_1but75uk01ess19052u2137912u87o.png" alt="image_1but75uk01ess19052u2137912u87o.png-18.8kB"></p><p>There is a simple mutual exclusion algorithm with no deadlock that uses one <code>test&amp;set</code> register(Algorithm 7).</p><p><img src="http://static.zybuluo.com/sqfan/77k1nnayun5530ikaodbasbx/image_1but7idl41nh27an18jv15541opb8i.png" alt="image_1but7idl41nh27an18jv15541opb8i.png-28.8kB"></p><ul><li>One processor could always grab V (i.e., win the test&amp;set competition) and starve the others. </li><li>No Lockout does not hold. </li><li>Thus Bounded Waiting does not hold.</li></ul><h3 id="Read-Modify-Write-Registersz"><a href="#Read-Modify-Write-Registersz" class="headerlink" title="Read-Modify-Write Registersz"></a>Read-Modify-Write Registersz</h3><p>The <code>RMW</code> operation read-modify-write <strong>all in one atomic operation</strong>.<br>Clearly, the <code>test&amp;set</code> operation is a special case of <code>rmw</code>, where f(V) = 1 for any V.<br><img src="http://static.zybuluo.com/sqfan/f2osslvatwb30n0pua4c8wo5/image_1but76ho2njp1kuvp1t1ujv1fki85.png" alt="image_1but76ho2njp1kuvp1t1ujv1fki85.png-13kB"></p><p><img src="http://static.zybuluo.com/sqfan/u54fzf0r8pav05wuv0nopeqh/image_1but836531d6418131m9isfn12h8v.png" alt="image_1but836531d6418131m9isfn12h8v.png-48.3kB"><br>Detailed analysis about algorithm 8 please refer to the textbook.</p><hr><h2 id="Mutual-Exclusion-Using-Read-Write-Registers"><a href="#Mutual-Exclusion-Using-Read-Write-Registers" class="headerlink" title="Mutual Exclusion Using Read/Write Registers"></a>Mutual Exclusion Using Read/Write Registers</h2><h3 id="The-Bakery-Algorithm"><a href="#The-Bakery-Algorithm" class="headerlink" title="The Bakery Algorithm"></a>The Bakery Algorithm</h3><p>Basic idea:</p><ul><li>Tell others “I want to enter the critical section”</li><li>Get tickets and wait for my turn</li></ul><p>We employ the following shared data structures:</p><ul><li><code>Number</code>: an array of n integers, which holds in its ith entry the number of p_i</li><li><code>Choosing</code>: an array of n Boolean values; Choosing[i] is <code>true</code> while p_i is in the process of obtaining its number.</li></ul><p>Because several processors can read <code>Number</code> concurrently it is possible for several procesors to obtain the same number!.</p><p>To break symmetry, we define <code>p_i&#39;s ticket</code> be the pair <code>(Number[i], i)</code> （uniqueness）(tickets之间的序比较可以使用字典序). </p><p><img src="http://static.zybuluo.com/sqfan/tdv2dfbu3ltig8y99gjxgdhk/image_1but8u55uj361l3m1qh5em71qvc9s.png" alt="image_1but8u55uj361l3m1qh5em71qvc9s.png-44.6kB"></p><p>Algorithm 10 provides mutual exclusion and no lockout. (proof pls refer to the textbook)</p><p>The numbers can <strong>grow without bound</strong>, unless when every processor is in the remainder section.</p><blockquote><p>How to achieve MUTEX when variables have finite size.</p></blockquote><h3 id="A-Bounded-Mutual-Exclusion-Algorithm-for-2-Processors"><a href="#A-Bounded-Mutual-Exclusion-Algorithm-for-2-Processors" class="headerlink" title="A Bounded Mutual Exclusion Algorithm for 2 Processors"></a>A Bounded Mutual Exclusion Algorithm for 2 Processors</h3><p><code>Algorithm 11</code> provides mututual exclusion and no deadlock for two processors p_0 and p_1. Each processor p_i has a Boolean shared variable <code>Want[i]</code>.</p><ul><li>Want[i]=1: if p_i wants to enter the critical section.</li></ul><p>However, the algorithm gives <strong>prioirty</strong> to one of the processors and the other one can <code>starve</code>.</p><p><img src="http://static.zybuluo.com/sqfan/1r7ll2awo5aubcelhlwicgnm/image_1butan78f14791ir1h7ot2f1ih1a9.png" alt="image_1butan78f14791ir1h7ot2f1ih1a9.png-54.2kB"></p><p>We then convert this algorithm to one that provides no lockout as well.<br>To achieve no lockout, we modify the alrgorithm so that instead of always giving priority to p_0, each processor gives priority to the other processor on leaving the critical section.</p><ul><li><code>Priority</code>: <strong>Shared variable</strong>, contains the id of the processor that has the priority at the moment.</li></ul><p>In the entry, wait until:</p><ul><li>Case 1: the other processor has the priority (but does not want to enter the critical section)</li><li><p>Case 2: I have the priority.</p><p>  <img src="http://static.zybuluo.com/sqfan/9yl17tapp29nu8oxbqc2tfxu/image_1butanrpuomi9ru1je1101o1pooam.png" alt="image_1butanrpuomi9ru1je1101o1pooam.png-76.4kB"></p></li></ul><h3 id="A-Bounded-Mutual-Exclusion-Algorithm-for-n-Processors-recursively-use-the-2-no-lockout-algorithm"><a href="#A-Bounded-Mutual-Exclusion-Algorithm-for-n-Processors-recursively-use-the-2-no-lockout-algorithm" class="headerlink" title="A Bounded Mutual Exclusion Algorithm for n Processors (recursively use the [2, no lockout] algorithm)"></a>A Bounded Mutual Exclusion Algorithm for n Processors (recursively use the [2, no lockout] algorithm)</h3><p>To construct a solution for the general case of <code>n</code> processors we <strong>employ the algorithm for two processors</strong>.</p><ul><li>Proceessors compete pairwise, using the two-processor algorithm described above, in a <code>tournament tree</code> (锦标赛树) arrangement.</li><li>The pairwise competitions are arranged in a complete binary tree.</li><li>Each processor begins at a specific leaf of the tree.</li><li>At each level, the winner gets to proceed to the next higher level, where it competes with the winner of the competition on the other side of the tree.</li><li><strong>The processor that wins at the root enters the critical section</strong>.</li></ul><p><img src="http://static.zybuluo.com/sqfan/knpj6h460t7gmkxllgi15jqj/image_1butapds11jguce1nt31dtp1q1qb3.png" alt="image_1butapds11jguce1nt31dtp1q1qb3.png-17.7kB"></p><ul><li><code>v</code>: node number; we associate shared variable <code>Want^v[0]</code>, <code>Want^v[1]</code>, and <code>Priority^v</code> (all initialized to 0)</li><li><p>To begin the competition for the (real) critical section, processor <code>p_i</code> executes <code>Node(2^k + i/2, i mod 2)</code></p><ul><li>note that the leaves of the tree are numbered 2^k, 2^k + 1, …, 2^(k+1) - 1 (see Fig. 4.7). </li><li><p>the processor on the left (right) side play the role of p_0 (p_1)</p><p><img src="http://static.zybuluo.com/sqfan/cm23hvnk0z7iihjtb8jvkayu/image_1butapvs01k681mh31j781dti6a9bg.png" alt="image_1butapvs01k681mh31j781dti6a9bg.png-56.2kB"></p></li></ul></li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] <a href="http://hagit.net.technion.ac.il/publications/dc/" target="_blank" rel="external">Attiya, Hagit, and Jennifer Welch. <strong>Distributed computing</strong>: fundamentals, simulations, and advanced topics. Vol. 19. John Wiley &amp; Sons, 2004.</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Preface&quot;&gt;&lt;a href=&quot;#Preface&quot; class=&quot;headerlink&quot; title=&quot;Preface&quot;&gt;&lt;/a&gt;Preface&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;The MUTEX problem&lt;ul&gt;
&lt;li&gt;the shared memory model&lt;/li&gt;
&lt;li&gt;problem definition&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The unbounded algorithm&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;bakery algorithm&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The unbounded algorithm&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Introduction to Distributed Algorithms" scheme="http://fanshiqing.github.io/categories/Introduction-to-Distributed-Algorithms/"/>
    
    
      <category term="分布式算法" scheme="http://fanshiqing.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>L03-Leader Election in Rings</title>
    <link href="http://fanshiqing.github.io/2017/10/12/L03-Leader-Election-in-Rings/"/>
    <id>http://fanshiqing.github.io/2017/10/12/L03-Leader-Election-in-Rings/</id>
    <published>2017-10-12T13:16:03.000Z</published>
    <updated>2017-11-13T06:39:24.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Discuss the leader election (LE) problem in message-passing systems for a ring topology, in which a group of processors must choose one among them to be a leader.</li><li>Present the different algorithms for leader election problem by taking the cases like anonymous/non-anonymous rings, uniform/non-uniform rings and synchronous/asynchronous rings etc.</li></ul><a id="more"></a><p>[Based on the book “<strong>Distributed Computing</strong>“ by Hagit attiya &amp; Jennifer Welch]</p><h2 id="Ring-Networks"><a href="#Ring-Networks" class="headerlink" title="Ring Networks"></a>Ring Networks</h2><p><img src="http://static.zybuluo.com/sqfan/qtrrif1ta6opgr54rx5071ij/image_1bue05vdhcjd1eb9rnt4do2spm.png" alt="image_1bue05vdhcjd1eb9rnt4do2spm.png-15.7kB"></p><ul><li>In an <code>oriented ring</code>, processors have a consistent notion of left and right.<ul><li>For example, if messages are always forwarded on <code>channel 1</code>, they will cycle clockwise around the ring.</li></ul></li></ul><blockquote><p>Why study rings?</p><ul><li>simple starting point, easy to analyze</li><li>abstraction of a token ring</li><li>lower bounds and impossibility results for ring topology also apply to arbitrary topoligies.</li></ul></blockquote><h2 id="The-Leader-Election-LE-Problem"><a href="#The-Leader-Election-LE-Problem" class="headerlink" title="The Leader Election (LE) Problem"></a>The Leader Election (LE) Problem</h2><ul><li>LE problem is for each processor to decide that either it is <strong>the leader or non-leader</strong>, subject to the constraint that exactly one processor decides to be the leader.</li><li>LE problem represents a general class of <strong>symmetry-breaking</strong> problems.<ul><li>For example, when a <code>deadlock</code> is created, because of processors waiting in a cycle for each other, the deadlock can be broken by electing one of the processor as a leader and removing it from the cycle. </li></ul></li><li>Each processor has a set of <code>elected (won)</code> and <code>not-elected (lost)</code> states.</li><li>Once an elected state is entered, processor is always in an elected state (and similarly for not-elected): i.e., irreversible decision.</li><li>In every admissible execution:<ul><li>every processor eventually enters an elected or a not-elected state</li><li>exectly one processor (the <code>leader</code>) enters an elected state.</li></ul></li></ul><h3 id="Uses-of-LE"><a href="#Uses-of-LE" class="headerlink" title="Uses of LE"></a>Uses of LE</h3><ul><li>A leader can be used to coordinate activities of the system:<ul><li>find a <code>spanning tree</code> using the leader as the root;</li><li>reconstruct a <code>lost token</code> in a token-ring network.</li></ul></li></ul><h2 id="Uniform-Anonymous-Algorithms"><a href="#Uniform-Anonymous-Algorithms" class="headerlink" title="Uniform (Anonymous) Algorithms"></a>Uniform (Anonymous) Algorithms</h2><ul><li><code>Anonymous or not</code>: A leader election algorithm is anonymous if processors do not have unique identifiers that can be used by the algorithm<ul><li>Message recipients can only be specified in terms of channel labels, e.g., left and right neighbors </li><li>==&gt; Every processor in the system has the same state machine.</li></ul></li><li>A <code>uniform</code> algorithm does not use the ring size (same algorithm for each size ring)<ul><li>Formally, every processor in every size ring is modeled with the same state machine</li><li>Uniform: since the algorithm looks the same for every value of n.</li></ul></li><li>A <code>non-uniform</code> algorithm uses the ring size (different algorithm for each size ring)<ul><li>Formally, for each value of n, every processor in a ring of size n is modeled with the same state machine An .</li></ul></li></ul><h2 id="Leader-Election-in-Anonymous-Rings"><a href="#Leader-Election-in-Anonymous-Rings" class="headerlink" title="Leader Election in Anonymous Rings"></a>Leader Election in Anonymous Rings</h2><p><code>Theorem:</code> <strong>For nonuniform algorithms and synchronous rings, there are no anonymous LE algorithms</strong>.</p><p>Proof Sketch:</p><ul><li>Every processor begins in same state with same outgoing msgs (since anonymous)</li><li>Every processor receives same msgs, does same state transition, and sends same msgs in round 1</li><li>Ditto for rounds 2,3,…</li><li>Eventually some processor is supposed to enter an elected state.  But then they all would.</li></ul><p>Proof sketch shows that either <strong>safety</strong> (never elect more than one leader) or <strong>liveness</strong> (eventually elect at least one leader) is violated. Since the theorem was proved for non-uniform and synchronous rings, the same result holds for weaker (less well-behaved) models (uniform / asynchronous).</p><p><img src="http://static.zybuluo.com/sqfan/rveobgknzovji6jdsubae73v/image_1bug0fadc1e523b21fn7uqu1qn830.png" alt="image_1bug0fadc1e523b21fn7uqu1qn830.png-95.2kB"></p><blockquote><p><a href="https://en.wikipedia.org/wiki/Lattice_(order" target="_blank" rel="external">Lattice</a>) (格）<br>A lattice is an abstract structure studied in the mathematical subdisciplines of order theory and abstract algebra. It consists of a partially ordered set in which every two elements have a unique supremum (上确界) (also called a least upper bound or <strong>join</strong>, a ∧ b) and a unique infimum （下确界）(also called a greatest lower bound or <strong>meet</strong>, a V b)</p></blockquote><p>Based on the impossibility result, we can reasonably assume that: <code>Rings with unique processor identifiers</code>.</p><h2 id="LE-in-Asynchronous-Rings"><a href="#LE-in-Asynchronous-Rings" class="headerlink" title="LE in Asynchronous Rings"></a>LE in Asynchronous Rings</h2><p>There exists algorithms when nodes have unique ids. We will evaluate them according to their <em>message complexity</em>.</p><h3 id="Brute-Force-LE"><a href="#Brute-Force-LE" class="headerlink" title="Brute Force LE"></a>Brute Force LE</h3><p><img src="http://static.zybuluo.com/sqfan/a2iw6dqxfppfi3o06tm3cftr/image_1bug5niima1fh3so2m1uua1jdp3d.png" alt="image_1bug5niima1fh3so2m1uua1jdp3d.png-16.1kB"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">Send value of own id to the left.</div><div class="line"></div><div class="line">When receive an id j (from the right)</div><div class="line">    if j &gt; id then</div><div class="line">        forward j to the left // this processor has lost</div><div class="line">    if j == id then</div><div class="line">        elect self // this processor has won</div><div class="line">    if j &lt; id then</div><div class="line">        // do nothing (swallow)</div></pre></td></tr></table></figure></p><h4 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h4><ul><li><strong>Correctness</strong>: Elect processor with the largest id. (Time: O(n))</li><li><strong>Message complexity</strong>: Depends on how the ids are arranged.<ul><li>largest id travels all around the ring (n msgs)</li><li>2nd largest id travels until reaching largest</li><li>3rd largest id travels until reaching largest or second largest.</li><li>etc.</li><li>Worst way to arrange the ids is in decreasing order (Fig.3.2)</li></ul></li></ul><p>The O(n^2) algorithm is simple and works in both sync and async model. But how to optimize?</p><blockquote><p><strong>Idea</strong>: try to have message containing smaller ids travel smaller distance in the ring.</p></blockquote><h3 id="k-neighbour-Forwarding"><a href="#k-neighbour-Forwarding" class="headerlink" title="k-neighbour Forwarding"></a>k-neighbour Forwarding</h3><ul><li>Basic idea<ul><li>Gradually increase the scope of sending</li><li>Eliminate unnecessary senders accordingly</li><li>Smaller IDs are swallowed</li></ul></li><li>Clever forwarding<ul><li>k-neighbourhood<ul><li>2k+1 nodes: k left + k right + self</li></ul></li><li>in the kth phase, LE among the 2^k-neighborhood<ul><li>size of neighbourhood <strong>doubles</strong> in each phase</li><li>only the winner survives to the next phase</li></ul></li></ul></li></ul><p><img src="http://static.zybuluo.com/sqfan/faxuqz0jm6fxsyxjj6w9qd1m/image_1bug88k2j1tj7109p19ll12e096b4a.png" alt="image_1bug88k2j1tj7109p19ll12e096b4a.png-197kB"></p><h4 id="Analysis-1"><a href="#Analysis-1" class="headerlink" title="Analysis"></a>Analysis</h4><ul><li>Correctness: similar to O(n*2) algorithm</li><li><strong>Message complexity</strong><ul><li>Each msg belongs to a particular phase and is initiated by a particular proc.</li><li>Probe distance in phase k is 2^k</li><li>Number of msgs initiated by a proc. in phase k is at most <strong>4</strong> * 2^k (probes and replies in both directions)</li><li>How many proc. initiate probes in phase <em>k</em> ?<ul><li>For k = 0, every proc. does</li><li>For k &gt; 0, every proc. that is a “winner” in previous phase (phase <strong>k-1</strong>) does</li></ul></li><li>Maximum number of phase <strong>k-1</strong> winners occurs when the are packed as densely as possible:<br><img src="http://static.zybuluo.com/sqfan/vl3eiohzz0fj2zt1y8g2y8yc/image_1bugc8ue119cphe614og1qjv1qrk4n.png" alt="image_1bugc8ue119cphe614og1qjv1qrk4n.png-12.5kB"></li><li>Total number of phase k-1 winners is at most <strong>n/(2^(k-1) + 1)</strong></li><li>How many phases are there?<ul><li>let <strong>n/2^(k-1) + 1 == 1</strong> (at the last phase there exists only one winner) ==&gt; <strong>k = log(n-1) + 1</strong> = number of phases</li></ul></li></ul></li></ul><p>So the total number of msgs is sum, over all phases, of number of winners at that phase <em>times</em> number of msgs originated by that winner:<br><img src="http://static.zybuluo.com/sqfan/fxnyl4pjmr8co6i8hh5onfz0/image_1bugd4jjg1kvm1ff117bbo541k1l54.png" alt="image_1bugd4jjg1kvm1ff117bbo541k1l54.png-25.7kB"></p><p>The O(log n) algorithm is more complicated than the O(n^2) algorithm but uses fewer messages in worst case.</p><h2 id="Lower-bound-for-LE-Algorithm"><a href="#Lower-bound-for-LE-Algorithm" class="headerlink" title="Lower bound for LE Algorithm"></a>Lower bound for LE Algorithm</h2><p>Can we do better than O(nlogn) ?</p><p><code>Theorem</code>: Any leader election algorithm for <strong>asynchronous</strong> rings whose size is not known a priori has Ω(nlog n) msg complexity (holds also for undirectional rings).</p><ul><li>The two algorithms above are <strong>comparison-based</strong> algorithms, i.e. they use the identifiers only for comparison (&lt;, &gt;, =)</li><li>In synchronous networks, O(n) msg complexity can be achieved if general arithmatic operations are permitted (<strong>non-comparison based</strong>) and if time complexity is unbounded.</li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] <a href="http://hagit.net.technion.ac.il/publications/dc/" target="_blank" rel="external">Attiya, Hagit, and Jennifer Welch. <strong>Distributed computing</strong>: fundamentals, simulations, and advanced topics. Vol. 19. John Wiley &amp; Sons, 2004.</a><br>[2] <a href="http://www.bigoh.net/wiki/index.php/Dis-alg" target="_blank" rel="external">分布式算法（黄宇）课程主页</a><br>[3] <a href="https://www.youtube.com/watch?v=LRUcPNet6i4" target="_blank" rel="external">Distributed System</a></p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;Discuss the leader election (LE) problem in message-passing systems for a ring topology, in which a group of processors must choose one among them to be a leader.&lt;/li&gt;
&lt;li&gt;Present the different algorithms for leader election problem by taking the cases like anonymous/non-anonymous rings, uniform/non-uniform rings and synchronous/asynchronous rings etc.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Introduction to Distributed Algorithms" scheme="http://fanshiqing.github.io/categories/Introduction-to-Distributed-Algorithms/"/>
    
    
      <category term="分布式算法" scheme="http://fanshiqing.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>L02-Basic Message Passing Algorithms</title>
    <link href="http://fanshiqing.github.io/2017/10/05/L02-Basic-Message-Passing-Algorithms/"/>
    <id>http://fanshiqing.github.io/2017/10/05/L02-Basic-Message-Passing-Algorithms/</id>
    <published>2017-10-05T08:23:50.000Z</published>
    <updated>2017-11-13T06:42:16.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Broadcast / convergecast on a spanning tree</li><li>Async / sync flooding to construct a spanning tree</li><li>distributed DFS with/without a specific root</li></ul><a id="more"></a><h2 id="Broadcast-over-a-rooted-spanning-tree"><a href="#Broadcast-over-a-rooted-spanning-tree" class="headerlink" title="Broadcast over a rooted spanning tree"></a>Broadcast over a rooted spanning tree</h2><ul><li>Broadcast is used to send the information to all.</li><li>Suppose processors <strong>already have information about a rooted spanning tree of the communication topology</strong><ul><li>tree: connected graph with no cycles</li><li>spanning tree: contains all processors</li><li>rooted: there is a unique root node</li></ul></li><li>Implemented via <code>parent</code> and <code>children</code> local varialbes at each processor.<ul><li>indicate which incident channnels lead to parent and children in the rooted spanning tree. </li></ul></li></ul><p><img src="http://static.zybuluo.com/sqfan/digj1gg45grlsp7yqlbxy387/image_1bu5gq7v33lgv161mc1lef1g9t9.png" alt="image_1bu5gq7v33lgv161mc1lef1g9t9.png-186.6kB"></p><blockquote><p><strong><a href="https://en.wikipedia.org/wiki/Spanning_tree" target="_blank" rel="external">Spanning Tree</a></strong>: A tree is a connected undirected graph with no cycles. It is a spanning tree of a graph G if it <strong>spans</strong> G (that is, it <code>includes every vertex of G</code>) and <code>is a subgraph of G</code> (every edge in the tree belongs to G). A spanning tree of a connected graph G can also be defined as a maximal set of edges of G that contains no cycle, or as a minimal set of edges that connect all vertices.</p></blockquote><p>Complexity analysis:</p><ul><li>Synchronous model<ul><li>Time complexity: time is depth <code>d</code> of the spanning tree. (at most <code>n-1</code> when chain)</li><li>MSG complexity: number of messages is <code>n-1</code>, since one message is sent over each spanning tree edges.</li></ul></li><li>Aysnchronous model<ul><li>Same as synchronous model. </li></ul></li></ul><h2 id="Convergecast-from-leaves-to-the-root"><a href="#Convergecast-from-leaves-to-the-root" class="headerlink" title="Convergecast (from leaves to the root)"></a>Convergecast (from leaves to the root)</h2><ul><li>Convergecast is used to collect the information.</li><li>Again, suppose a rooted spanning tree has already been computed by the processors<ul><li><em>parent</em> and <em>children</em> variables at each processor</li></ul></li><li>Do the opposite of broadcast<ul><li>leaves send msgs to their parents.</li><li>non-leaves wait to get msgs from each child, then send <strong>combined</strong> (aggregate) info to parent.</li></ul></li></ul><h2 id="Finding-a-Spanning-Tree-Given-a-Root-by-Flooding"><a href="#Finding-a-Spanning-Tree-Given-a-Root-by-Flooding" class="headerlink" title="Finding a Spanning Tree Given a Root by Flooding"></a>Finding a Spanning Tree Given a Root by Flooding</h2><blockquote><p><a href="https://en.wikipedia.org/wiki/Flooding_(computer_networking" target="_blank" rel="external">Flooding</a>): Flooding is a simple computer network routing algorithm in which every incoming packet is sent through every outgoing link except the one it arrived on.</p><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b1/Flooding_routing.gif/220px-Flooding_routing.gif" alt="flooding"></p></blockquote><ul><li>root send <em>M</em> to all its neighbours</li><li>when non-root first gets <em>M</em>,<ul><li>set the sender as its parent</li><li>send “parent” msg to sender</li><li>send <em>M</em> to all other neighbours (if no other neighours, then terminate)</li></ul></li><li>when get <em>M</em> otherwise,<ul><li>send “reject” to sender.</li></ul></li><li>use “parent” and “reject” msgs to set <em>children</em> varialbes and know when to terminate (after hearing from all neighbours)</li></ul><p><img src="http://static.zybuluo.com/sqfan/xdwf7bfg97pumdx3j45v8trt/image_1bu5ifmh212aq1a9oauomuchdn16.png" alt="image_1bu5ifmh212aq1a9oauomuchdn16.png-83.1kB"></p><p>Execution of spanning tree algorithm</p><ul><li>In the synchronous model: always gives breadth-first search (BFS) tree.</li><li>Asynchronous: not necessarily BFS tree. </li></ul><p>Both models achieves O(m) messages complexity and O(diam) time complexity.</p><blockquote><p><strong>Diameter <code>D</code> of a network</strong> is defined as the longest path of the shortest paths between any two nodes.</p></blockquote><h2 id="Distributed-DFS-with-a-Specified-Root"><a href="#Distributed-DFS-with-a-Specified-Root" class="headerlink" title="Distributed DFS with a Specified Root"></a>Distributed DFS with a Specified Root</h2><ul><li>Basic rationale: <strong>sequential execution</strong> over a distributed system (of multiple processors)</li></ul><p><img src="http://static.zybuluo.com/sqfan/6j4qhtilocj6kqjq3cusu57p/image_1bu5jha1n20n14gm0p1u3c198i20.png" alt="image_1bu5jha1n20n14gm0p1u3c198i20.png-281.1kB"></p><h2 id="Distributed-DFS-without-a-Specified-Root"><a href="#Distributed-DFS-without-a-Specified-Root" class="headerlink" title="Distributed DFS without a Specified Root"></a>Distributed DFS without a Specified Root</h2><ul><li>Assume <strong>the processors have unique identifiers</strong> (otherwise impossible!)</li><li>Idea:<ul><li>Each processor starts running a copy of the DFS spanning tree algorithm, with itself as root</li><li><strong>tag each msg with initiator’s id to differentiate</strong></li><li>when copies “collide”, copy with <code>larger id wins</code>.</li></ul></li><li>Message complexity: O(n*m)</li><li>Time complexity: O(m) (m: edges in graph)</li></ul><p><img src="http://static.zybuluo.com/sqfan/sb92i2ocjs27079je9k9axtg/image_1bu5k3agccbb1no917v3iak19gk2d.png" alt="image_1bu5k3agccbb1no917v3iak19gk2d.png-81kB"></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] <a href="http://hagit.net.technion.ac.il/publications/dc/" target="_blank" rel="external">Attiya, Hagit, and Jennifer Welch. <strong>Distributed computing</strong>: fundamentals, simulations, and advanced topics. Vol. 19. John Wiley &amp; Sons, 2004.</a><br>[2] <a href="http://www.bigoh.net/wiki/index.php/Dis-alg" target="_blank" rel="external">分布式算法（黄宇）课程主页</a><br>[3] <a href="https://www.youtube.com/watch?v=kqTkbEgREYk" target="_blank" rel="external">Distributed System</a></p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;Broadcast / convergecast on a spanning tree&lt;/li&gt;
&lt;li&gt;Async / sync flooding to construct a spanning tree&lt;/li&gt;
&lt;li&gt;distributed DFS with/without a specific root&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Introduction to Distributed Algorithms" scheme="http://fanshiqing.github.io/categories/Introduction-to-Distributed-Algorithms/"/>
    
    
      <category term="分布式算法" scheme="http://fanshiqing.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>L01-Model of Computation</title>
    <link href="http://fanshiqing.github.io/2017/09/27/L01-Model-of-Computation/"/>
    <id>http://fanshiqing.github.io/2017/09/27/L01-Model-of-Computation/</id>
    <published>2017-09-27T12:19:32.000Z</published>
    <updated>2017-11-13T06:46:23.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Async/sync system</li><li>Random access machine model</li><li>Message passing model</li><li>Shared memory model</li></ul><a id="more"></a><h2 id="Essential-Issues-of-Alogrithm"><a href="#Essential-Issues-of-Alogrithm" class="headerlink" title="Essential Issues of Alogrithm"></a>Essential Issues of Alogrithm</h2><ul><li>Model of computation</li><li>Algorithm design</li><li>Algorithm analysis</li></ul><h2 id="Asynchronous-amp-Synchronous-System"><a href="#Asynchronous-amp-Synchronous-System" class="headerlink" title="Asynchronous &amp; Synchronous System"></a>Asynchronous &amp; Synchronous System</h2><blockquote><p><strong>Asynchronous System</strong>. A system is said to be asynchronous if there is no fixed upper bound on how long it takes for a message to be delivered (message delays) or how much time elapses between consectutive steps of a processor (processor step time) [1].</p><p><strong>Synchronous System</strong>. In the synchronous model processors execute in lockstep: The execution is partitioned into rounds, and in each round, every processor can send a message to each neighbour, the messages are delivered, and every processor computes based on the messages just received. (This model is convenient for designing algorithms) [1]</p></blockquote><p><strong>Why asynchronous systems</strong>?</p><ul><li>Sometimes the upper bounds are quite large, are infrequently reached and change over time.</li><li>It is often desiable to design an algorithm that is <code>independent of any particular timing parameters</code>, namely an asynchronous algorithm<ul><li>Instead of design an algorithms that depends on the bounds </li></ul></li></ul><h2 id="Random-Access-Machine-RAM-Model"><a href="#Random-Access-Machine-RAM-Model" class="headerlink" title="Random Access Machine (RAM) Model"></a>Random Access Machine (<strong>RAM</strong>) Model</h2><p>The goal of working with a <strong>model</strong> computer instead of a <strong>real</strong> computer is that we want to have a machine, which is as easy as possible, but still let us capture the main aspects of a real computer.</p><p>This model of computation is an abstraction that allows us to <strong>compare algorithms on the basis of performance</strong>. Simplifications for RAM model:</p><ul><li>Simple operations take only 1 time step;</li><li>Loops and subroutines are not simple operations;</li><li>We assume we have as much memory as we need (<strong>infinite storage</strong>);</li><li>Memory access is considered to be free in terms of time (or one time step?);</li><li>A unit of memory cannot hold an arbitrarily large number. </li></ul><p><img src="http://static.zybuluo.com/sqfan/y88ifg3v7sobz4s4vdhvegub/image_1btlmrsqq14c81l3ks2f5j5n2b9.png" alt="RAM Model"><br>The RAM model takes no notice of whether an item is in cache or on the disk, which simplifies the analysis. It is an excellent model for understanding how an algorithm will perform on a real computer. It strikes a fine balance by <strong>capturing the essential behavior of computers while being simple to work with</strong>. We use the RAM model because it is useful in practice.</p><blockquote><p><strong>Relationship between the Turing Machine and RAM Models</strong><br>A random-access machine with unbounded indirection is equivalent to a Turing machine. Informally speaking, both machines have the same computational capabilities. (<a href="https://en.wikipedia.org/wiki/Random-access_machine#Turing_equivalence_of_the_RAM_with_indirection" target="_blank" rel="external">wikipedia</a> | <a href="https://www.youtube.com/watch?v=XhkXUHLVNtY" target="_blank" rel="external">Equivalance of RAM and Turing Machines</a>)</p></blockquote><h2 id="Message-Passing-Model"><a href="#Message-Passing-Model" class="headerlink" title="Message Passing Model"></a>Message Passing Model</h2><p>The architecture is used to communicate data among a set of processors without the need for a global memory. Each processor has its own local memory and communicates with other Processors using message.</p><p>Data exchanged among processors cannot be shared; it is rather <strong>copied</strong> (using send/receive messages). An important advantage of this form of data exchange is the elimination of the need for synchronization constructs, such as semaphores, which results in performance improvement.</p><hr><h2 id="Shared-Memory-Model"><a href="#Shared-Memory-Model" class="headerlink" title="Shared Memory Model"></a>Shared Memory Model</h2><p>Both SMP and DSM are <strong>shared address space</strong> platforms.</p><h3 id="Symmetric-Multiprocessors-SMP"><a href="#Symmetric-Multiprocessors-SMP" class="headerlink" title="Symmetric Multiprocessors (SMP)"></a>Symmetric Multiprocessors (SMP)</h3><p>Processors all connected to a large shared memory. Examples are processors connected by crossbar, or multicore chips. It is <strong>symmetric</strong> because the access time from any of the CPUs to memory is the same.</p><p>Key characteristics is <strong>uniform memory acess</strong> (UMA).<br><img src="http://static.zybuluo.com/sqfan/m94fg25wndazqj6t2xsudhkm/image_1btmomv42gua1lo01eoj5duvos9.png" alt="SMP"><br><strong>Caches</strong> are a problem: need to be kept coherrent = when one CPU changes a value in memory, then all other CPUs will get the same value when they access it. All caches will show a coherent value. </p><hr><h3 id="Distributed-Shared-Memory-DSM"><a href="#Distributed-Shared-Memory-DSM" class="headerlink" title="Distributed Shared Memory (DSM)"></a>Distributed Shared Memory (DSM)</h3><p>DSM is basically an abstraction that integrates the local memory of different machine into a single logical entity shared by cooperating processes.</p><ul><li>The distributed shared memory implements the shared memory model in distributed systems, which have no physical shared memory. (shared memory exists only virtually, similar concepts to virtual memory)</li><li>The shared memory model provides a <strong>virtual address space</strong> shared between all nodes</li><li>The overcome the high cost of communication in distributed systems, DSM systems move data to the location of access.</li></ul><p><img src="http://static.zybuluo.com/sqfan/9pay9edubua92wzyii7axxqw/image_1btmqsj1c17d9ltl4h1nmm7o13.png" alt="image_1btmqsj1c17d9ltl4h1nmm7o13.png-26.3kB"></p><p><img src="http://static.zybuluo.com/sqfan/0eed7sgn0qpo255jag7onj1o/image_1btmrqvvr9hs33n1rt619pe1sgv1g.png" alt="image_1btmrqvvr9hs33n1rt619pe1sgv1g.png-342.3kB"></p><p>How?</p><ul><li>Data moves between main memory and secondary memory (within a node) and between main memories of different nodes.</li><li>Each data object is owned by a node<ul><li>Initial owner is the node that created object</li><li>Ownership can change as object moves from node to node</li></ul></li><li>When a process accesses data in the shared address space, the mapping manager maps shared memory address to physical memory (local or remote). </li></ul><h2 id="Shared-Memory-v-s-Message-Passing"><a href="#Shared-Memory-v-s-Message-Passing" class="headerlink" title="Shared Memory v.s. Message Passing"></a><a href="https://www.youtube.com/watch?v=SmJpGl_PYz8" target="_blank" rel="external">Shared Memory v.s. Message Passing</a></h2><table><thead><tr><th></th><th>Messsge Passing</th><th>Shared Memory</th></tr></thead><tbody><tr><td>who does communication</td><td>Programmer</td><td>Automatic</td></tr><tr><td>Data distribution</td><td>Manual</td><td>Automatic</td></tr><tr><td>HW support</td><td>Simple</td><td>Extensive (automatically figures out when to send data, to whom and where to cache in, etc.)</td></tr><tr><td>Programming</td><td></td><td></td></tr><tr><td>Correctness</td><td>Difficult</td><td>Less Difficult</td></tr><tr><td>Performance</td><td>Difficult (noce you get correctness, performance is not far away)</td><td>Very Difficult</td></tr></tbody></table><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] <a href="http://hagit.net.technion.ac.il/publications/dc/" target="_blank" rel="external">Attiya, Hagit, and Jennifer Welch. <strong>Distributed computing</strong>: fundamentals, simulations, and advanced topics. Vol. 19. John Wiley &amp; Sons, 2004.</a><br>[2] <a href="https://github.com/fanshiqing/Qix/blob/master/ds.md" target="_blank" rel="external">分布式系统(Distributed System)资料</a><br>[3] <a href="http://www.cs.nyu.edu/courses/fall10/G22.2945-001/slides/lect3-4.pdf" target="_blank" rel="external">Shared Memory, NYU Computer Science</a><br>[4] <a href="http://www.bigoh.net/wiki/index.php/Dis-alg" target="_blank" rel="external">分布式算法（黄宇）课程主页</a><br>[5] <a href="https://www.youtube.com/watch?v=SmJpGl_PYz8" target="_blank" rel="external">Message Passing Vs Shared Memory - Georgia Tech - HPCA: Part 5</a></p>]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;Async/sync system&lt;/li&gt;
&lt;li&gt;Random access machine model&lt;/li&gt;
&lt;li&gt;Message passing model&lt;/li&gt;
&lt;li&gt;Shared memory model&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Introduction to Distributed Algorithms" scheme="http://fanshiqing.github.io/categories/Introduction-to-Distributed-Algorithms/"/>
    
    
      <category term="分布式算法" scheme="http://fanshiqing.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>htop - interactive process viewer</title>
    <link href="http://fanshiqing.github.io/2017/06/18/htop/"/>
    <id>http://fanshiqing.github.io/2017/06/18/htop/</id>
    <published>2017-06-18T06:42:01.000Z</published>
    <updated>2017-07-04T08:00:01.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="htop命令简介"><a href="#htop命令简介" class="headerlink" title="htop命令简介"></a>htop命令简介</h1><p><code>htop</code>是Linux系统下的一个<strong>交互式</strong>、<strong>实时</strong>进程监控应用程序，<code>top</code>的高级版。</p><p>优点：</p><ul><li>可以横向或纵向滚动浏览进程列表，以便查看所有的进程和<strong>完整的命令行</strong></li><li>支持<strong>鼠标操作</strong></li><li>杀进程时不需要输入进程号(快捷键: <code>F9</code>)</li></ul><h2 id="htop-vs-top"><a href="#htop-vs-top" class="headerlink" title="htop vs top"></a>htop vs top</h2><blockquote><p>It is similar to top, but allows you to scroll vertically and horizon-<br>       tally, so you can see all the processes running on the  system,  along<br>       with  their  full  command lines, as well as viewing them as a process<br>       tree, selecting multiple processes and acting on them all at once.<br>Tasks related to processes (killing, renicing)  can  be  done  without<br>       entering their PIDs. (摘自<code>htop</code>手册:)</p></blockquote><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>For Mac OS:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ brew install htop</div></pre></td></tr></table></figure><p>For Ubuntu 14.04 LTS:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo apt-get install htop</div></pre></td></tr></table></figure><p>查看命令手册：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ man htop</div></pre></td></tr></table></figure><p>启动htop：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ htop</div></pre></td></tr></table></figure><p>将得到如下类似的一个实时进程监控窗口：<br><img src="https://ooo.0o0.ooo/2017/07/03/5959f95e6c77f.png" alt="htop panel"></p><p>帮助：按<code>F1</code>进入使用帮助。</p><p>退出：按下<code>q</code>键退出htop面板。</p><h1 id="监控面板介绍"><a href="#监控面板介绍" class="headerlink" title="监控面板介绍"></a>监控面板介绍</h1><h2 id="系统CPU使用率"><a href="#系统CPU使用率" class="headerlink" title="系统CPU使用率"></a>系统CPU使用率</h2><p><img src="https://ooo.0o0.ooo/2017/07/03/595a490e9ef34.png" alt="cpu-usage"></p><p>图中上半部分中的数字<code>1</code>到<code>8</code>表示系统中CPU/Core的数量，而紧邻数字的右侧进度条则相应地表示了对应CPU/Core的实时负载。进度条中不同颜色具有不同的含义<font color="grey">(以下为默认配置，具体操作时可以在<code>F2-&gt;Setup-&gt;Colors</code>列表中选择不同的主题)</font>：</p><ul><li><strong>CPU使用率栏</strong>: [<font color="blue">低优先级进程/</font><font color="green">用户进程/</font><font color="red">内核进程</font> &ensp; &ensp; &ensp; &ensp; &ensp; &ensp; &ensp; &ensp; &ensp; &ensp; <font color="grey">used%</font>]</li></ul><h2 id="系统内存使用率"><a href="#系统内存使用率" class="headerlink" title="系统内存使用率"></a>系统内存使用率</h2><ul><li><strong>Memory使用率栏</strong>：[<font color="green">已使用/</font><font color="blue">buffers/</font><font color="orange">cache</font> &ensp; &ensp; &ensp; &ensp; &ensp; &ensp; &ensp; &ensp; &ensp; &ensp; <font color="grey">used/total</font>]</li></ul><h2 id="系统平均负载"><a href="#系统平均负载" class="headerlink" title="系统平均负载"></a>系统平均负载</h2><p><img src="https://ooo.0o0.ooo/2017/07/03/595a45a4a111c.png" alt="load average"></p><ul><li><strong>负载</strong>(<a href="https://en.wikipedia.org/wiki/Load_(computing" target="_blank" rel="external">Load</a>)): 运行队列(run-queue)的长度：L = 等待进程的数目 + 运行进程的数目</li><li><strong>平均负载</strong>(<a href="https://www.howtogeek.com/194642/understanding-the-load-average-on-linux-and-other-unix-like-systems/" target="_blank" rel="external">load average</a>) 在<strong>一段时间内</strong>CPU正在处理以及等待CPU处理的进程数之和的统计信息，也就是CPU使用队列的长度的统计信息<blockquote><p><code>“最大负载=核心数”法则</code> 在多核处理中，负载不应该高于<strong>处理器核心的总数量</strong>。<font color="grey"><a href="http://blog.scoutapp.com/articles/2009/07/31/understanding-load-averages" target="_blank" rel="external">source</a></font></p></blockquote></li></ul><p>上图中<code>Load average</code>之后的3个数字显示的是系统在<code>1分钟</code>，<code>5分钟</code>，<code>15分钟</code>之内的平均负载值。(注： <code>uptime</code>命令可以直接查看load average)</p><h2 id="进程详细实时信息列表"><a href="#进程详细实时信息列表" class="headerlink" title="进程详细实时信息列表"></a>进程详细实时信息列表</h2><p><img src="https://ooo.0o0.ooo/2017/07/03/595a48b21eaa3.png" alt="processes-info"></p><p>每一列依次表示：</p><ul><li><strong>PID</strong> 进程ID</li><li><strong>USER</strong> 进程的所有者</li><li><strong>PRI</strong> 进程优先级。数字越小，优先级越高。</li><li><strong>NI</strong> 进程的<code>nice</code>值（负值表示高优先级，正值表示低优先级）</li><li><strong>VIRT</strong> 进程使用的虚拟内存</li><li><strong>RES</strong> 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA</li><li><strong>S</strong> 进程状态<ul><li><font color="green">R</font> (Running) &ensp; &ensp; 可执行状态（<font color="red">运行/就绪</font>)</li><li>D (uninterruptedly sleeping) &ensp; &ensp; <font color="red">不可中断的睡眠状态</font>.通常是在等待IO，比如磁盘IO，网络IO，其他外设IO (该状态不接收外来的任何信号，因此无法用kill杀掉D状态的进程)</li><li>S (sleeping) &ensp; &ensp; <font color="red">可中断的睡眠状态</font>(因为等待某某事件的发生(比如等待socket连接、等待信号量),而被挂起)</li><li>T (traced) &ensp; &ensp; 暂停状态或跟踪状态 (例如在<code>gdb</code>中对被跟踪的进程下一个断点，进程在断点处停下来的时候就处于task_traced状态)</li><li>Z (zombied) &ensp; &ensp; 退出状态，进程成为僵尸进程(已经结束了的进程，但是没有从进程表中删除)</li></ul></li><li><font color="red"><strong>CPU%</strong></font> 进程的CPU时间片利用率</li><li><font color="red"><strong>MEM%</strong></font> 进程的物理内存利用率</li><li><strong>TIME+</strong> 进程使用的处理器时间总计</li><li><font color="red"><strong>Command</strong></font> 启动该进程的完整命令行</li></ul><p>最后一行是F1~F10的功能菜单和对应的字母快捷键。</p><h1 id="Read-More"><a href="#Read-More" class="headerlink" title="Read More"></a>Read More</h1><p><a href="https://www.howtogeek.com/194642/understanding-the-load-average-on-linux-and-other-unix-like-systems/" target="_blank" rel="external">Understanding the Load Average on Linux and Other Unix-like Systems</a><br><a href="http://blog.scoutapp.com/articles/2009/07/31/understanding-load-averages" target="_blank" rel="external">Understanding Linux CPU Load - when should you be worried?</a><br><a href="http://www.deonsworld.co.za/2012/12/20/understanding-and-using-htop-monitor-system-resources/" target="_blank" rel="external">Understanding and using htop to monitor system resources</a><br><a href="http://www.brendangregg.com/Slides/Velocity2015_LinuxPerfTools.pdf" target="_blank" rel="external">Linux    Performance Tools</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;htop命令简介&quot;&gt;&lt;a href=&quot;#htop命令简介&quot; class=&quot;headerlink&quot; title=&quot;htop命令简介&quot;&gt;&lt;/a&gt;htop命令简介&lt;/h1&gt;&lt;p&gt;&lt;code&gt;htop&lt;/code&gt;是Linux系统下的一个&lt;strong&gt;交互式&lt;/stro
      
    
    </summary>
    
      <category term="Linux性能分析工具" scheme="http://fanshiqing.github.io/categories/Linux%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="Linux" scheme="http://fanshiqing.github.io/tags/Linux/"/>
    
      <category term="Linux性能分析" scheme="http://fanshiqing.github.io/tags/Linux%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>算法基础</title>
    <link href="http://fanshiqing.github.io/2017/06/15/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"/>
    <id>http://fanshiqing.github.io/2017/06/15/算法基础/</id>
    <published>2017-06-15T13:59:17.000Z</published>
    <updated>2017-08-17T01:29:29.000Z</updated>
    
    <content type="html"><![CDATA[<script src=/js/crypto-js.js></script><script>function doDecrypt(pwd, onError) {console.log("in doDecrypt");var txt = document.getElementById("enc_content").innerHTML;var plantext;try {  var bytes = CryptoJS.AES.decrypt(txt, pwd);plaintext = bytes.toString(CryptoJS.enc.Utf8);} catch(err) {if(onError) {onError(err);}return;}document.getElementById("enc_content").innerHTML = plaintext;document.getElementById("enc_content").style.display = "block";   document.getElementById("enc_passwd").style.display = "none";}</script><div id="enc_content" style="display:none">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</div><div id="enc_passwd"> <input id="enc_pwd_input" type="text" style="border-radius: 5px;border-style: groove;height: 30px;width: 50%;cursor: auto;font-size: 102%;color: currentColor;outline: none;text-overflow: initial;padding-left: 5px;" onkeydown="if (event.keyCode == 13) { decrypt(); return false;}"> <input type="submit" value="解&nbsp;密" onclick="decrypt()" style="width: 58px;height: 34px;border-radius: 5px;background-color: white;border-style: solid;color: currentColor;"><div id="enc_error" style="display: inline-block;color: #d84527;margin-left: 10px"></div><script>var onError = function(error) {document.getElementById("enc_error").innerHTML = "password error!"};function decrypt() {var passwd = document.getElementById("enc_pwd_input").value;console.log(passwd);doDecrypt(passwd, onError);}</script></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script src=/js/crypto-js.js&gt;&lt;/script&gt;&lt;script&gt;
function doDecrypt(pwd, onError) {
	console.log(&quot;in doDecrypt&quot;);
	var txt = document.getEleme
      
    
    </summary>
    
    
      <category term="算法" scheme="http://fanshiqing.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>【闲情】记 第二届南大上海校友会新年羽毛球嘉年华</title>
    <link href="http://fanshiqing.github.io/2017/01/07/%E9%97%B2%E6%83%85-%E8%AE%B0%E7%AC%AC%E4%BA%8C%E5%B1%8A%E5%8D%97%E5%A4%A7%E4%B8%8A%E6%B5%B7%E6%A0%A1%E5%8F%8B%E4%BC%9A%E6%96%B0%E5%B9%B4%E7%BE%BD%E6%AF%9B%E7%90%83%E5%98%89%E5%B9%B4%E5%8D%8E/"/>
    <id>http://fanshiqing.github.io/2017/01/07/闲情-记第二届南大上海校友会新年羽毛球嘉年华/</id>
    <published>2017-01-07T14:00:00.000Z</published>
    <updated>2017-07-09T10:57:25.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><ul><li>时间： 2017年1月7日</li><li>地点：上海财大（武川路校区）羽毛球馆（旧馆）</li><li>事件：第二届南大上海校友会新年羽毛球嘉年华活动</li><li>角色：第二次参加这个活动，本次担任 <font color="red">杨浦队</font> <strong>队长</strong> ^_^</li><li>赛制：趣味赛 + 团体赛</li><li><p>赛果：杨浦队在8支参赛队中勇夺 <strong><em>第4名</em></strong> (历史最好成绩)！</p><blockquote><ul><li>趣味赛:<ol><li>单手颠球，穿越障碍</li><li>三人隔网，你发我接（限时）</li></ol></li><li>团体赛(8支)<ul><li>徐汇、闵松、沿江、张江、花木、<font color="red"><strong>杨浦</strong></font>、长静黄联队、市北联队</li></ul></li></ul></blockquote></li><li><p>赛后聚餐地点：上海财大豪生酒店</p></li></ul><h4 id="赛前宣传海报-感谢-陈雨致-学姐的精心设计-："><a href="#赛前宣传海报-感谢-陈雨致-学姐的精心设计-：" class="headerlink" title="赛前宣传海报(感谢@陈雨致 学姐的精心设计)："></a>赛前宣传海报(感谢@陈雨致 学姐的精心设计)：</h4><p><img src="/2017/01/07/闲情-记第二届南大上海校友会新年羽毛球嘉年华/比赛海报.jpeg" alt="比赛海报"><br><img src="/2017/01/07/闲情-记第二届南大上海校友会新年羽毛球嘉年华/各队队长.jpeg" alt=""></p><h4 id="赛前集体合影-每队一列，列首为该队队长"><a href="#赛前集体合影-每队一列，列首为该队队长" class="headerlink" title="赛前集体合影(每队一列，列首为该队队长):"></a>赛前集体合影(每队一列，列首为该队队长):</h4><p><img src="/2017/01/07/闲情-记第二届南大上海校友会新年羽毛球嘉年华/赛前全体合影.JPG" alt=""><br><img src="/2017/01/07/闲情-记第二届南大上海校友会新年羽毛球嘉年华/大合影.jpeg" alt=""></p><h4 id="高质量的比赛场馆-感谢-王绍立学长大力帮助联系和提供场地！"><a href="#高质量的比赛场馆-感谢-王绍立学长大力帮助联系和提供场地！" class="headerlink" title="高质量的比赛场馆(感谢@王绍立学长大力帮助联系和提供场地！):"></a>高质量的比赛场馆(感谢@王绍立学长大力帮助联系和提供场地！):</h4><p><img src="/2017/01/07/闲情-记第二届南大上海校友会新年羽毛球嘉年华/比赛场地.jpeg" alt=""></p><h4 id="比赛中："><a href="#比赛中：" class="headerlink" title="比赛中："></a>比赛中：</h4><p><img src="/2017/01/07/闲情-记第二届南大上海校友会新年羽毛球嘉年华/比赛中.jpeg" alt=""><br><img src="/2017/01/07/闲情-记第二届南大上海校友会新年羽毛球嘉年华/小宇宙组合.jpeg" alt=""></p><h4 id="赛果"><a href="#赛果" class="headerlink" title="赛果:"></a>赛果:</h4><ul><li>冠军：闵松队(根据约定，本次比赛冠军将负责主办明年的第三届比赛^_-)</li><li>亚军：沿江队</li><li>季军：张江队</li><li>殿军：<font color="red">杨浦队</font></li></ul><p><img src="/2017/01/07/闲情-记第二届南大上海校友会新年羽毛球嘉年华/赛果.jpeg" alt=""></p><h4 id="赛后各队队长、裁判、志愿者们合影"><a href="#赛后各队队长、裁判、志愿者们合影" class="headerlink" title="赛后各队队长、裁判、志愿者们合影:"></a>赛后各队队长、裁判、志愿者们合影:</h4><p><img src="/2017/01/07/闲情-记第二届南大上海校友会新年羽毛球嘉年华/志愿者.jpeg" alt=""></p><h4 id="大杨浦队赛后合影留念"><a href="#大杨浦队赛后合影留念" class="headerlink" title="大杨浦队赛后合影留念"></a>大杨浦队赛后合影留念</h4><p><img src="/2017/01/07/闲情-记第二届南大上海校友会新年羽毛球嘉年华/yangpu1.jpeg" alt=""><br><img src="/2017/01/07/闲情-记第二届南大上海校友会新年羽毛球嘉年华/yangpu2.jpeg" alt=""></p><h4 id="赛后聚餐"><a href="#赛后聚餐" class="headerlink" title="赛后聚餐:"></a>赛后聚餐:</h4><p><img src="/2017/01/07/闲情-记第二届南大上海校友会新年羽毛球嘉年华/赛后聚餐.jpeg" alt=""></p><h4 id="广告时间"><a href="#广告时间" class="headerlink" title="广告时间"></a>广告时间</h4><p>感谢<font color="red">炜觉资本</font>对本次活动的大力赞助！</p><hr><p>明年再来，<font color="red">闵松</font>主办！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;时间： 2017年1月7日&lt;/li&gt;
&lt;li&gt;地点：上海财大（武川路校区）羽毛球
      
    
    </summary>
    
      <category term="闲情" scheme="http://fanshiqing.github.io/categories/%E9%97%B2%E6%83%85/"/>
    
    
      <category term="闲情" scheme="http://fanshiqing.github.io/tags/%E9%97%B2%E6%83%85/"/>
    
      <category term="羽毛球" scheme="http://fanshiqing.github.io/tags/%E7%BE%BD%E6%AF%9B%E7%90%83/"/>
    
      <category term="相册" scheme="http://fanshiqing.github.io/tags/%E7%9B%B8%E5%86%8C/"/>
    
  </entry>
  
  <entry>
    <title>CaffeNet vs CaffeOnSpark</title>
    <link href="http://fanshiqing.github.io/2016/05/12/CaffeNet-vs-CaffeOnSpark/"/>
    <id>http://fanshiqing.github.io/2016/05/12/CaffeNet-vs-CaffeOnSpark/</id>
    <published>2016-05-12T05:28:20.000Z</published>
    <updated>2017-07-04T06:16:14.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SparkNet"><a href="#SparkNet" class="headerlink" title="SparkNet"></a>SparkNet</h1><p><a href="https://github.com/amplab/SparkNet" target="_blank" rel="external">SparkNet</a> 是由amplab开发的基于Spark的分布式深度神经网络架构，2015年11月份在github上开源. 该系统开发的动机在于当下流行的批处理计算框架（MapReduce、Spark等）都不是设计用来支持已有的分布式深度学习系统下的异步的、通行密集型的任务。SparkNet将已有的数据处理框架（Spark)和流行的、高效的深度学习框架Caffe整合到一起，同时提供了Spark RDDs和Caffe的访问接口；并通过改进的并行化SGD算法来降低节点间的通信开销[1].</p><p>SparkNet的架构示例如<code>图1</code>所示[1]。</p><p><img src="http://ww3.sinaimg.cn/large/006HJ39wgy1fh7t47ans2j30f4068wfl.jpg" alt="SparkNet-Archi"></p><p><code>图1</code>是由5个节点组成的EC2集群。其中Master节点负责向其余4各worker节点分发任务；每个worker在本地单独使用基于GPU的caffe来进行训练；本地训练结束后各个worker将参数回传给Master;Master收集各个worker的参数后做全局的处理（例如average）再broadcast给各个worker以进行下一轮的迭代。</p><h1 id="CaffeOnSpark"><a href="#CaffeOnSpark" class="headerlink" title="CaffeOnSpark"></a>CaffeOnSpark</h1><p><a href="https://github.com/yahoo/CaffeOnSpark" target="_blank" rel="external">CaffeOnSpark</a> 由Yahoo开发，并于今年2月份开源在github上。该系统旨在将深度学习步骤(训练、测试等)无缝地嵌入到Spark应用中，使得直接在存储数据的(强化的)Hadoop集群上进行深度学习成为可能，从而避免了数据在Hadoop集群和深度学习集群之间不必要的传输. 作为Spark的深度学习包，CaffeOnSpark填补了<code>Spark MLlib</code>在DL能力上的不足[3]。可以这样说，CaffeOnSpark集合了Caffe和Spark两者之长，并应用到大规模深度学习上，使得我们能像操作其他Spark应用一样操作深度学习任务。</p><p>CaffeOnSpark的系统架构图如<code>图2</code>[6] 所示：</p><p><img src="http://ww1.sinaimg.cn/large/006HJ39wgy1fh7t5uf29fj30ff0bb76e.jpg" alt=""><br>&ensp;&ensp; &ensp; &ensp; &ensp; &ensp; &ensp;&ensp; &ensp; &ensp; &ensp; &ensp; &ensp;<strong>图2. CaffeOnSpark System Architecture</strong></p><p>从图2中我们看到CaffeOnSpark在<strong>参数的同步/管理</strong>操作上和SparkNet是完全不同的：CaffeOnSpark executors之间通过<code>MPI_ALLREDUCE</code>接口通信（通过底层<code>RDMA/Infiniband</code> 或者TCP/Ethenet来保证GPU间高速的数据传输,10x于CPU).在这种通信模式下，各个节点/Caffe引擎之间是<strong>peer-to-peer</strong>的模式；而<code>SparkNet</code>的设计中依然保留了Spark的<strong>主/从模式</strong>。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>CaffeOnSpark和SparkNet的相同之处在于两者都使得Spark集群下基于Caffe的深度学习成为可能。</p><p>而两者最显著的不同在于两者在系统框架设计中对于参数同步所采用的不同方案。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>[1]. Moritz,Philipp, et al. “SparkNet: Training Deep Networks in Spark.” arXiv preprint arXiv:1511.06051 (2015).<br>[2]. <a href="https://www.slideshare.net/HadoopSummit/distributed-deep-learning-on-hadoop-clusters" target="_blank" rel="external">Large Scale Distributed Deep Learning on Hadoop Clusters, PPT</a><br>[3]. GitHub - amplab/SparkNet, <a href="https://github.com/amplab/SparkNet" target="_blank" rel="external">https://github.com/amplab/SparkNet</a><br>[4]. GitHub - yahoo/CaffeOnSpark, <a href="https://github.com/yahoo/CaffeOnSpark" target="_blank" rel="external">https://github.com/yahoo/CaffeOnSpark</a><br>[5]. CaffeOnSpark Google Group, <a href="https://groups.google.com/forum/#!forum/sparknet-users" target="_blank" rel="external">https://groups.google.com/forum/#!forum/sparknet-users</a><br>[6]. <a href="http://www.deepminder.com/?p=2647" target="_blank" rel="external">Large Scale Distributed Deep Learning on Hadoop Clusters</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;SparkNet&quot;&gt;&lt;a href=&quot;#SparkNet&quot; class=&quot;headerlink&quot; title=&quot;SparkNet&quot;&gt;&lt;/a&gt;SparkNet&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/amplab/SparkNet&quot; t
      
    
    </summary>
    
      <category term="深度学习" scheme="http://fanshiqing.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习框架" scheme="http://fanshiqing.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/"/>
    
  </entry>
  
</feed>
