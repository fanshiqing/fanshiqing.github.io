<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[分布式一致性算法——Paxos简介]]></title>
    <url>%2F2017%2F11%2F23%2F%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94Paxos%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[背景介绍在分布式系统中，一个核心的问题就是数据一致性问题。Paxos一致性算法是分布式系统中的经典算法，由分布式大师Lamport提出，用来解决一个分布式系统中如何就某个决议（值）达成一致的问题，基于消息传递且具有高度容错性。用好理解的方式说，就是在一个选举过程中，让不同的选民最终做出一致的决定。Lamport本人也因其对分布式系统的杰出理论贡献获得了2013年的图灵奖。 Paxos is a mechanism for achieving consensus on a single value over unreliable communication channels. Paxos算法分析 分布式一致性问题(The distributed consensus problem） ConsistencyGet multiple servers to agree on state. Consistenty类型 Strong: 强一致性。要求无论更新操作是在哪一个副本执行，之后所有的读操作都要能获得最新的数据（对用户最友好，但是性能影响大）; Weak:弱一致性。用户读到某一操作对系统特定数据的更新需要一段时间，我们称这段时间为“不一致性窗口”。但经过“不一致时间窗口”这段时间后，后续对该数据的读取都是更新后的值 (best effort only, used for cache); Eventually最终一致性。是弱一致性的一种特例，保证用户最终能够读取到某操作对系统特定数据的更新(DNS是典型的最终一致性系统)。 在分布式系统中，我们如何能够在多个选项中选择一个单独的动作（action）呢？又该如何让这个动作能以容错（fault-tolerant）的方式来完成呢？这是分布式共识问题的基本问题。这个问题听起来似乎很简单：只需要选择一个节点来做决定嘛！但显然这会引入单点失效问题：如果这个唯一的决策者失效了，那所有其他节点就没有办法知道该采取哪个行动了（这其实违反了接下来要介绍的Paxos原则之一的存活性原则）。那在决策者失效时重新选择一个决策者会如何？但这可能会导致最终有两个决策者，而每个决策者可能做出不同的决定，导致共识不能达成。好吧这个问题似乎不那么简单。让我们来看看Paxos是如何解决这个问题的。 两个原则（requirements）}现在我们已经对这个一致性问题有了直觉上的认识，下面来阐述解决方案的两个原则：安全原则和存活原则。 安全原则 (safety)：系统不会产生“错误”的答案 只有被提议（proposed）的值才可能被选中（chosen) 只有一个值被选中 一个节点永远不会知道一个值已经被选择，除非它本身已经被选中 （一旦达成共识，这个事实被公之于众，且不可更改(回退？)） 存活原则 (liveness)：只要多数节点存活并且彼此间可以通信，最终都要做到的事情（正常运行） 某个被提议的值最终将被选中 如果某个值已经被选中，那么其他节点最终可以学习到这个值 基本假设（assumptions）除了上述的两条原则之外，我们还需要对分布式系统的工作原理以及可能失效的情形作出一些假设。分布式系统中的节点通信主要存在两种模型：共享内存和消息传递。Paxos算法基于消息传递。在基于消息传递的分布式系统中，不可避免地会出现消息延迟、丢失、重复的情形。此外，在消息传递模型中，节点故障可以分为失效停止（fail stop）和拜占庭错误（Byzantine Failure）（所有不是失效-停止的错误都可以归为拜占庭错误）两大类，基础Paxos算法不考虑拜占庭将军问题。更具体地，基础Paxos算法的基本假设具体如下。 失效-停止 假设 失败-停止： 当一个节点发生故障时，它将完全停止工作 重启：故障的节点重新启动时可以恢复正常操作 消息（messages）相关的假设：基于消息传递的异步通信 消息可以丢失 消息可以重复发送 消息可以延迟(只要等待足够的时间，消息就会被送到)（乱序到达）但是消息不会被篡改（即不考虑拜占庭错误） 稳定的存储：假定系统中的节点可以访问某种形式的稳定的存储器，这些存储器可以保存故障之前记录的信息，直到节点重启并从故障中恢复（为什么需要稳定的存储？因为每个节点以任意的速度异步操作，可能因为fail stop而重启。所以任何一个节点都可能会在提议被选择后停机再重启，因此解决方案要求节点必须要能够记忆某些信息，从而能在重启后重新载入） 基本概念在《Paxos made simple》论文描述中将一致性算法中节点的角色分类3类：proposers, acceptors和learners。不同的角色对应于一致性算法中的不同职责。更具体的，算法描叙相关的基本概念如下： proposal value: 提议的值 proposal number: 提议编号 proposal: 提议（= 提议编号 + 提议值） proposor: 提议发起者，提出不同的建议值（proposoal）供决策者考虑。提议是&lt;N, V&gt;对 acceptor： 提议接收者，负责达成共识。（接受符合一定限制的提案） learner：最终决策学习者，不参与提议的处理，学习提议的处理结果。 实际实现中，一个节点往往同时扮演这多个角色（通常是3个）。当在某个提议值上达成共识时我们就说这个值被选出来了。简单起见，在下面的算法分析中，我们只考虑Basic Paxos算法（没有learner这个角色）。 提议编号Paxos能在不可靠的通信信道上进行操作的关键之一是为每个proposal分配一个唯一的ID。Proposal ID是什么以及为什么重要的细节将在稍后带来，这里我们先将这些ID视为与时间戳（timestamps）类似的角色：它们不仅让peers能从逻辑上判断一条消息是否比另一条消息更新，而且绕开了分布式系统中一致的、共享的时间概念（notions of time）所固有的复杂性。 Proposal ID的常见实现是一个简单的整数和一个用于确保唯一性的一个“tie-breaker”组成数对。这里使用的ID的pair的组合模式是 &lt; Integer, 节点的ID（具有唯一性）&gt;， 例如，（1，A）。这些ID是可比较的。为了确定哪个ID在逻辑上更新，第一个整数首先以正常的方式进行比较，并且如果它们相等，则将这些唯一的ID字符串进行字典序比较以打破不分胜负的局面，因而有（5，B）&gt;（4，B）&gt;（4，A）。 ID的主要目的是保护算法免受延迟和/或重复的消息的影响。 Paxos使用一个非常简单的规则来实现这一点。如果新收到的消息的ID大于上一个处理的消息的ID，则对其进行处理。否则忽略这条消息。忽略延迟信息的动机并不像忽略重复消息那样显而易见，其原因来自于在Paxos这样一个多步骤的算法中, 与旧的和过期的step相关的消息不被允许干扰当前active step的消息。 Paxos算法流程Paxos算法具体分为两个阶段。 第一阶段（prepare）： 提议者选择一个全局唯一的建议值n，并向多数的（过半）接受者（majority of acceptors）发送一个准备请求（prepare request）：prepare(n)。如果一个接受者接收到一个值为n的准备请求，且该请求值比它看到的任何请求值都大，那么它对该请求作出确认答复promise(n)，承诺不再接收任何小于这个n的提议（并且包括它已经接收了的编号最高的提案（如果有的话）） 第二阶段（accept）： 如果proposer收到过半的acceptor的确认消息表示他们不会接受n以下的提议, 那么这个proposer给这些acceptor返回附带他的建议值v的确认消息（Accept(n, v)）. 如果一个acceptor收到这样的确认消息{n, v}, acceptor就会接受它(Accepted(n, v)). （除非在此期间acceptor又收到了一个比n更大的提议.） Basic Paxos算法流程具体流程如下图： 这里还需要对一个提议值被选择的概念做一个阐述：一个提议值n被选择当且仅当在上述算法的第二阶段超过半数的接收者接收了该值。这里有必要对Paxos算法里的这个majority这个概念多加阐述。 强大多数（Strong Majority/Quorum): 一组由一半以上的接受者组成的集合。 通过上面的描述可知，在节点间达成共识的关键思想就是这个“强大多数”的概念。这个概念的特殊之处在于：任意两个“强大多数”的集合都至少有一个共同的成员节点。Paxos算法通过让提议者向“强大多数”的接收者发送提议来利用这个特性：如果我们可以得到超过半数的节点来对某个单一的提议值达成共识，那么我们已经达成一致；因为所有其他Quorum将包含至少一个此Quorum的节点，而交集内的进程就可以保证正确性被继承，以后被传播出去。同时，这也意味着我们一次可以容忍的错误节点的数量： 若正常工作的节点小于等于半数个节点，就不可能形成多数。 那为什么要分为两个阶段呢？先说一下prepare阶段的作用。第一，检查是有已经有被批准的值，如果有的话，就用被批准的值。第二，如果之前存在提议还没有被接受，则干掉他们（以便不让他们和我们发生竞争）。 第一阶段中proposal ID (即prepare()发送的n)有什么特性呢？Paxos算法要求Proposal ID全局唯一且递增。为什么要这样设计呢？这个ID实际上相当于一个虚拟时钟（virtual clock), 所有的acceptors都会使用这个ID，用来保证当它收到任意节点发来的消息时，该消息是最新的（或者说是目前处于Paxos算法最新一轮的消息）而不是过时的消息（系统异步环境下消息的允许任意延迟）。事实上，全局唯一ID（且递增）的生成本身是需要一定的技术来保证的：毕竟需要同时满足高并发、高可用、低延迟这样的需求可不简单。微信的seqvr系统就是专门用来做这种序列生成的。 是不是算法已经完全没有问题了呢？根据上述过程，当一个proposer发现存在编号更大的提案时将终止提案。这意味着提出一个编号更大的提案会终止之前的提案过程。如果两个节点在这种情况下都转而提出一个编号更大的提案，就可能陷入活锁（live lock），违背了Progress的原则。这种情况下的解决方案是选举出一个leader（用我们之前学的leader election知识），仅允许leader提出提案。但是由于消息传递的不确定性（异步通信模型），可能有多个proposer自认为自己已经成为leader（也就是没有达成谁是leader的consensus）。 一个示例（不考虑failures）2个proposor, 3个acceptor, 1个learner. Phase 1: Prepare/Promise Proposor A和Proposal B分别向3个acceptors发送准备请求prepare(2)和prepare(4). Prepare(2)率先到达Acceptor X,Y. 这两个acceptor向A作出确认答复Promise(0,null)。Prepare(4)率先到达Acceptor Z. Z向B作出确认答复Promise(0,null)。 Proposor B的Prepare(4)之后于A到达Acceptor X,Y. X,Y检查发现该请求值（4）比它看到的任何请求值都大（最大是之前A发来的prepare(2)）,那么它对该请求作出确认答复promise(2,null)。 Proposor A的Prepare(2)之后于B到达Acceptor Z.因为请求值（2）小于之前处理的最大请求值4，该消息被忽略。 Phase 2: Accept/Accepted Proposal A收到了2/3(超过半数)的promise消息，于是给这些acceptor返回附带他的建议值v的确认消息Accept(2,100). 因为在phase1之后所有的acceptor都看到了目前最大的promise值是B发来的4，所以A本次发给acceptors的accept消息都将被忽略。 Proposal B也收到了3/3(超过半数)的promise消息，于是给这些acceptor返回附带他的建议值v的确认消息Accept(4,50). Acceptor X，Y, Z检查本地最大提议值和Accept的ID相同，于是将写入本地，并返回Accepted（）消息表示确认接受。Learner会学习到这一情况并把value=50被选择的情况记录下来，并告知proposer A. Paxos的属性Paxos的属性论文里有详细列出，为了完整起见，我还是列在这里，完整的推导过程可以参见维基百科上的Paxos算法。 P1：一个acceptor必须接受（accept）第一次收到的提案。 P2：一旦一个具有value v的提案被批准（chosen），那么之后批准（chosen）的提案必须具有value v。 P2a：一旦一个具有value v的提案被批准（chosen），那么之后任何acceptor再次接受（accept）的提案必须具有value v。 P2b：一旦一个具有value v的提案被批准（chosen），那么以后任何proposer提出的提案必须具有value v。 P2c：如果一个编号为n的提案具有value v，那么存在一个多数派，要么他们中所有人都没有接受（accept）编号小于n的任何提案，要么他们已经接受（accept）的所有编号小于n的提案中编号最大的那个提案具有value v。 Paxos算法主要贡献Paxos算法是第一个被证明的一致性算法，也是最著名的分布式一致性算法。Google的粗粒度锁服务Chubby的底层一致性实现就是以Paxos算法为基础。用户只需调用Chubby的锁服务接口就能够实现分布式系统中多个进程之间粗粒度的同步控制，从而保证分布式数据的一致性。Chubby的开发者对Paxos曾有过这样的评价：“所有的一致性协议本质上要么是Paxos，要么就是其变体”。 小结Paxos算法可以说是分布式系统中最重要的一个算法。虽然Lamport在《Paxos made simple》中声称“Paxos is among the simplest and most obvious of distributed algorithms”, 但是个人在仔细研读几遍这篇论文后，还是觉得文章还是比较晦涩难懂的，虽然全文没有涉及到一个数学公式，都是文字描述，但是也没有直观的图来说明，最后还是查阅了其他各路资料，其中Chris Colohan在油管上一个关于paxos的lectture帮助很大。感觉Paxos最难理解的地方就在于要能搞清楚是什么因素导致该协议以这种方式呈现，而非其最终协议本身的内容。结合具体的应用场景来理解Paxos或许更好一点。 在本篇讨论中，实际上是讨论了Basic Paxos算法。而Basic Paxos只是理论模型，实际中使用的性能上更好的Multi-Paxos。Multi Paxos基于Basic Paxos, 将原来的2阶段过程（至少需要2次网络交互 + 2次本地持久化）简化为1个阶段，从而加速了提交速度。但是由于Multi-Paxos在理解和实现的复杂性，另一个改进版的一致性协议——Raft应运而生。Raft相比Paxos而言更加健全，更加易懂，虽然其在核心协议上基本是继承了Paxos中基于多数派的协议，但是其核心的贡献在于定义了易于实现的分布式一致性协议的事实标准。从某种意义上说，Raft不能算是一个新的协议，而是Paxos的一个具体化和简单化版本。 参考资料[1] Lamport, Leslie. “Paxos made simple.” ACM Sigact News 32.4 (2001): 18-25.[2] Paxos lecture by Chris Colohan, video[3] Understanding Paxos, blog[4] Tutorial Summary: Paxos Explained from Scratch, pdf[5] Paxos by example, ppt]]></content>
      <categories>
        <category>Introduction to Distributed Algorithms</category>
      </categories>
      <tags>
        <tag>分布式算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L05-Fault-tolerant consensus]]></title>
    <url>%2F2017%2F10%2F26%2FL05-Fault-tolerant-consensus%2F</url>
    <content type="text"><![CDATA[Preface The consensus problem Consensus with Byzantine Failures Processor Failures in Message Passing Fail stop: at some point the processor stops taking steps at the processor’s final step, it might succeed in sending only a subset of the messages it is supposed to send, e.g.: Crash Power outage Hardware failure Out of memory/disk full Strategies: Checkpoint state and restart (High latency) Replicate state and fail over (失效备援) (High cost) Byzantine: Everything that is not fail stop. Processor changes state arbitrarily and sends message with arbitrary content. E.g.: Bit flip in memory or on disk corrupts data Older version of code on one node sends (now) invalid messages Node starts running malicious version of software Goal: turn into fail stop Checksums/ECC (Error Correction Code) Assertions Timeouts Failure Matrix Negative result for link failuresIt is impossible to reach consensus in case of link failures, even in the synchronous case, and even if one only wants to tolerate a single link failure. Consensus under link failures: the 2 generals problem. There are two generals of the same army who have encamped a short distance apart. Their objective is to capture a hill, which is possible only if they attack simultaneously. If only one general attacks, he will be defeated. The two generals can only communicate by sending messengers, which is not reliable. Is it possible for them to attack simultaneously? See Fischer/Lynch/Paterson for more discussion. The Consensus ProblemEach process starts with an individual input from a particular value set V. Processes may fail by crashing. All non-faulty processes are required to produce outputs from the value set V, subject to simple agreement and validity. A solution to the consensus problem must guarantee the following: Termination: Eventually every nonfaulty processor must decide on a value (decision is irrevocable). Agreement: All decisions by nonfaulty processors must be the same. Validity: If all inputs are the same, then the decision of a nonfaulty processor must equal the common input Once a processor crashes, it is of no interest to the algorithm, and no requirements are placed on its decision. f-resilient system: at most f processors may fail The set of faulty processors may be different in different executions. In the last round: Clean crash: none or all of the outgoing messages are sent Not-clean crash: an arbitrary set of its outgoing messages are delivered. Consensus algorithm in the presence of crash failuresEach process maintains a set of the values it knows to exist in the system; initially, this set contains only its own input. At the first round, each process broadcasts its own input to all processes. For the subsequent f rounds, each process takes the following actions: updates its set by joining it with the sets received from otherprocesses, and broadcasts any new additions to the set to all processes. After f+1 rounds, the process decides on the smallest value in its set. Intuition for Agreement: Assume that a process pi decides on a value x smaller than that decided by some other process p_j. Then, x has remained “hidden” from pj for (f+1) rounds. We have at most f faulty processes. A contradiction!!! Number of processes: n &gt; f Round Complexity: f + 1 Message Complexity: (at most) n^2 * |V| messages, where V is the set of input values. Worst case scenario Consensus with Byzantine Failures Theorem (5.7): Any consensus algorithm for 1 Byzantine failure must have at least 4 processors. Proof of Theorem 5.7: Theorem: Any consensus algorithm for f Byzantine failures must have at least 3f+1 processors. Partition the processors into three sets P0, P1, P2; Each containing at most n/3 processors P0 simulates p0, P1 simulates p1 and P2 simulates p2 n processors solves consensus =&gt; {p0, p1, p2} solves consensus. ==&gt; Contradiction. Exponential Information Gathering (EIG) AlgorithmThis algorithm uses f + 1 rounds (optimal) n = 3f + 1 processors (optimal) exponential size messages (sub-optimal) Each processor keeps a tree data structure in its local state. Values are filled in the tree during the f + 1 roundsAt the end, the values in the tree are used to calculate the decision. Local Tree Data Structure Each tree node is labeled with a sequence of unique processor indices. Root’s label is empty sequence ; root has level 0 Root has n children, labeled 0 through n - 1 Child node labeled i has n - 1 children, labeled i : 0 through i : n-1 (skipping i : i) Node at level d labeled v has n - d children, labeled v : 0 through v : n-1 (skipping any index appearing in v) Nodes at level f + 1 are leaves. The tree when n = 4 and f = 1 : Filling in the Tree Nodes Initially store your input in the root (level 0) Round 1: send level 0 of your tree to all store value x received from each p_j in tree node labeled j (level 1); use a default if necessary “pj told me that pj ‘s input was x” Round 2: send level 1 of your tree to all store value x received from each pj for each tree node k in tree node labeled k : j (level 2); use a default if necessary “pj told me that pk told pj that pk’s input was x” Continue for f + 1 rounds Calculating the Decision In round f + 1, each processor uses the values in its tree to compute its decision. Recursively compute the “resolved” value for the root of the tree, resolve(), based on the “resolved” values for the other tree nodes: Example of resolving values when n = 4 and f = 1: A Polynomial Algorithm (The King Algorithm)We can reduce the message size to polynomial with a simple algorithm. The number of processors increases to: n &gt; 4f The number of rounds increases to 2(f + 1) Uses f+1 phases, each taking two rounds. Asynchronous ConsensusAssumptions Communication system is reliable Only processor failures (crash / Byzantine) Completely asynchronous Theorem: For n ≥ 2, there is no algorithm in the read/write shared memory model that solves the agreement problem and guarantees wait-free termination Consensus is impossible!!! Even in the presence of one singleprocessor (crash) failure Proof of impossibility Impossibility for shared memory The (n-1)-resilient case (wait-free case) The 1-resilient case Impossibility for message passing Simulation Impossibility motivates the use of failure detectors. E.g., “The weakest failure detector for solving consensus”, JACM 43(4). SummaryLet f be the maximum number of faulty processors. References[1] Attiya, Hagit, and Jennifer Welch. Distributed computing: fundamentals, simulations, and advanced topics. Vol. 19. John Wiley &amp; Sons, 2004..[2] Byzantine Fault Tolerance, Distributed System (A free online class), by Chris Colohan.[3] The Byzantine Generals Problem, Leslie Lamport, Robert Shostack and Mashall Peace. ACM TOPLAS 4.3, 1982.[4] CSCE 668: Distributed Algorithms and Systems Spring 2014.]]></content>
      <categories>
        <category>Introduction to Distributed Algorithms</category>
      </categories>
      <tags>
        <tag>分布式算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L04-Mutual Exclusion]]></title>
    <url>%2F2017%2F10%2F19%2FL04-Mutual-Exclusion%2F</url>
    <content type="text"><![CDATA[Preface The MUTEX problem the shared memory model problem definition The unbounded algorithm The bakery algorithm The unbounded algorithm Shared Memory ModelProcesses communicate via a set of shared variables (also called shared registers), each shared variable has a type, defining a set of primitive operations (performed atomically) Several types of shared variable can be employed, e.g. read/write read-modify-write (RMW) compare&amp;swap (CAS) Each register has a type, which specifies: Values to be taken on by registers Operations performed on the registers Values to be returned by operations (if any) New values of the register resulting from the operation A configuration in the shared memory model is a vector: C = where q_i is a state of p_i and r_j is a value of register R. The events in a shared memory system are: computation steps taken by the processors and are denoted by the index of the processor; At each computation step, the shared variable is accessed. In asynchronous shared memory systems, an execution is admissible if each processor has an infinite number of computation steps. Complexity measuresObviously in shared memory systems there are no messages to measure. Instead we focus on the space complexity, the amount of shared memory needed to solve problems. Number of distinct shared variables required and the amount of shared space (e.g., # of bits) Changes from the MSG model Communication medium changes No inbuf and outbuf state components Configuration includes values for shared variables Execution manner changes One event type: one computation step by a process pi’s state in old configuration specifies which shared variable is to be accessed and with which primitive shared variable’s value in the new configuration changes according to the primitive’s semantics pi’s state in the new configuration changes according to its old state and the result of the primitive The Mutual Exclusion ProblemEach processor’s code is divided into four sections: Entry (trying): the code executed in preparation for entering the critical section Critical: the code to be protected from concurrent execution Exit (release): the code executed on leaving the critical section Remainder: the rest of the code Each processor cycles through these sections in the order: remainder –&gt; entry –&gt; critical –&gt; exit –&gt; remainder. An algorithm for a shared memory system solves the mutual exclusion problem with no deadlock (or no lockout) if the following holds (three properties): Mutual exclusion: In every configuration of every execution, at most one processor is in the critical section. No deadlock: In every admissible execution, if some processor is in the entry section in a configuration, then there is a later configuration in which some processor is in the critical section. No lockout: In every admissible execution, if some processor is in the entry section in a configuration, then there is a later configuration in which that same processor is in the critical section. Mutex progress conditions: no deadlock no lockout bounded waiting: no lockout + while a processor is in its entry section, other processors enter the critical section no more than a certain number of times. These three conditions are increasingly strong. The code for the entry and exit sections is allowed to assume that: no processor stays in its critical section forever shared variables used in the entry and exit sections are not accessed during the critical and remainder sections Mutual Exclusion Using Powerful PrimitivesWe will show that one bit suffices for guaranteeing mutual exclusion with no deadlock while O(logn) bits are necessary (and sufficient) for providing stronger fairness properties. Binary Test&amp;Set RegistersThe test&amp;set operation atomically reads and updates the variable. There is a simple mutual exclusion algorithm with no deadlock that uses one test&amp;set register(Algorithm 7). One processor could always grab V (i.e., win the test&amp;set competition) and starve the others. No Lockout does not hold. Thus Bounded Waiting does not hold. Read-Modify-Write RegisterszThe RMW operation read-modify-write all in one atomic operation.Clearly, the test&amp;set operation is a special case of rmw, where f(V) = 1 for any V. Detailed analysis about algorithm 8 please refer to the textbook. Mutual Exclusion Using Read/Write RegistersThe Bakery AlgorithmBasic idea: Tell others “I want to enter the critical section” Get tickets and wait for my turn We employ the following shared data structures: Number: an array of n integers, which holds in its ith entry the number of p_i Choosing: an array of n Boolean values; Choosing[i] is true while p_i is in the process of obtaining its number. Because several processors can read Number concurrently it is possible for several procesors to obtain the same number!. To break symmetry, we define p_i&#39;s ticket be the pair (Number[i], i) （uniqueness）(tickets之间的序比较可以使用字典序). Algorithm 10 provides mutual exclusion and no lockout. (proof pls refer to the textbook) The numbers can grow without bound, unless when every processor is in the remainder section. How to achieve MUTEX when variables have finite size. A Bounded Mutual Exclusion Algorithm for 2 ProcessorsAlgorithm 11 provides mututual exclusion and no deadlock for two processors p_0 and p_1. Each processor p_i has a Boolean shared variable Want[i]. Want[i]=1: if p_i wants to enter the critical section. However, the algorithm gives prioirty to one of the processors and the other one can starve. We then convert this algorithm to one that provides no lockout as well.To achieve no lockout, we modify the alrgorithm so that instead of always giving priority to p_0, each processor gives priority to the other processor on leaving the critical section. Priority: Shared variable, contains the id of the processor that has the priority at the moment. In the entry, wait until: Case 1: the other processor has the priority (but does not want to enter the critical section) Case 2: I have the priority. A Bounded Mutual Exclusion Algorithm for n Processors (recursively use the [2, no lockout] algorithm)To construct a solution for the general case of n processors we employ the algorithm for two processors. Proceessors compete pairwise, using the two-processor algorithm described above, in a tournament tree (锦标赛树) arrangement. The pairwise competitions are arranged in a complete binary tree. Each processor begins at a specific leaf of the tree. At each level, the winner gets to proceed to the next higher level, where it competes with the winner of the competition on the other side of the tree. The processor that wins at the root enters the critical section. v: node number; we associate shared variable Want^v[0], Want^v[1], and Priority^v (all initialized to 0) To begin the competition for the (real) critical section, processor p_i executes Node(2^k + i/2, i mod 2) note that the leaves of the tree are numbered 2^k, 2^k + 1, …, 2^(k+1) - 1 (see Fig. 4.7). the processor on the left (right) side play the role of p_0 (p_1) References[1] Attiya, Hagit, and Jennifer Welch. Distributed computing: fundamentals, simulations, and advanced topics. Vol. 19. John Wiley &amp; Sons, 2004.]]></content>
      <categories>
        <category>Introduction to Distributed Algorithms</category>
      </categories>
      <tags>
        <tag>分布式算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L03-Leader Election in Rings]]></title>
    <url>%2F2017%2F10%2F12%2FL03-Leader-Election-in-Rings%2F</url>
    <content type="text"><![CDATA[Discuss the leader election (LE) problem in message-passing systems for a ring topology, in which a group of processors must choose one among them to be a leader. Present the different algorithms for leader election problem by taking the cases like anonymous/non-anonymous rings, uniform/non-uniform rings and synchronous/asynchronous rings etc. [Based on the book “Distributed Computing“ by Hagit attiya &amp; Jennifer Welch] Ring Networks In an oriented ring, processors have a consistent notion of left and right. For example, if messages are always forwarded on channel 1, they will cycle clockwise around the ring. Why study rings? simple starting point, easy to analyze abstraction of a token ring lower bounds and impossibility results for ring topology also apply to arbitrary topoligies. The Leader Election (LE) Problem LE problem is for each processor to decide that either it is the leader or non-leader, subject to the constraint that exactly one processor decides to be the leader. LE problem represents a general class of symmetry-breaking problems. For example, when a deadlock is created, because of processors waiting in a cycle for each other, the deadlock can be broken by electing one of the processor as a leader and removing it from the cycle. Each processor has a set of elected (won) and not-elected (lost) states. Once an elected state is entered, processor is always in an elected state (and similarly for not-elected): i.e., irreversible decision. In every admissible execution: every processor eventually enters an elected or a not-elected state exectly one processor (the leader) enters an elected state. Uses of LE A leader can be used to coordinate activities of the system: find a spanning tree using the leader as the root; reconstruct a lost token in a token-ring network. Uniform (Anonymous) Algorithms Anonymous or not: A leader election algorithm is anonymous if processors do not have unique identifiers that can be used by the algorithm Message recipients can only be specified in terms of channel labels, e.g., left and right neighbors ==&gt; Every processor in the system has the same state machine. A uniform algorithm does not use the ring size (same algorithm for each size ring) Formally, every processor in every size ring is modeled with the same state machine Uniform: since the algorithm looks the same for every value of n. A non-uniform algorithm uses the ring size (different algorithm for each size ring) Formally, for each value of n, every processor in a ring of size n is modeled with the same state machine An . Leader Election in Anonymous RingsTheorem: For nonuniform algorithms and synchronous rings, there are no anonymous LE algorithms. Proof Sketch: Every processor begins in same state with same outgoing msgs (since anonymous) Every processor receives same msgs, does same state transition, and sends same msgs in round 1 Ditto for rounds 2,3,… Eventually some processor is supposed to enter an elected state. But then they all would. Proof sketch shows that either safety (never elect more than one leader) or liveness (eventually elect at least one leader) is violated. Since the theorem was proved for non-uniform and synchronous rings, the same result holds for weaker (less well-behaved) models (uniform / asynchronous). Lattice) (格）A lattice is an abstract structure studied in the mathematical subdisciplines of order theory and abstract algebra. It consists of a partially ordered set in which every two elements have a unique supremum (上确界) (also called a least upper bound or join, a ∧ b) and a unique infimum （下确界）(also called a greatest lower bound or meet, a V b) Based on the impossibility result, we can reasonably assume that: Rings with unique processor identifiers. LE in Asynchronous RingsThere exists algorithms when nodes have unique ids. We will evaluate them according to their message complexity. Brute Force LE123456789Send value of own id to the left.When receive an id j (from the right) if j &gt; id then forward j to the left // this processor has lost if j == id then elect self // this processor has won if j &lt; id then // do nothing (swallow) Analysis Correctness: Elect processor with the largest id. (Time: O(n)) Message complexity: Depends on how the ids are arranged. largest id travels all around the ring (n msgs) 2nd largest id travels until reaching largest 3rd largest id travels until reaching largest or second largest. etc. Worst way to arrange the ids is in decreasing order (Fig.3.2) The O(n^2) algorithm is simple and works in both sync and async model. But how to optimize? Idea: try to have message containing smaller ids travel smaller distance in the ring. k-neighbour Forwarding Basic idea Gradually increase the scope of sending Eliminate unnecessary senders accordingly Smaller IDs are swallowed Clever forwarding k-neighbourhood 2k+1 nodes: k left + k right + self in the kth phase, LE among the 2^k-neighborhood size of neighbourhood doubles in each phase only the winner survives to the next phase Analysis Correctness: similar to O(n*2) algorithm Message complexity Each msg belongs to a particular phase and is initiated by a particular proc. Probe distance in phase k is 2^k Number of msgs initiated by a proc. in phase k is at most 4 * 2^k (probes and replies in both directions) How many proc. initiate probes in phase k ? For k = 0, every proc. does For k &gt; 0, every proc. that is a “winner” in previous phase (phase k-1) does Maximum number of phase k-1 winners occurs when the are packed as densely as possible: Total number of phase k-1 winners is at most n/(2^(k-1) + 1) How many phases are there? let n/2^(k-1) + 1 == 1 (at the last phase there exists only one winner) ==&gt; k = log(n-1) + 1 = number of phases So the total number of msgs is sum, over all phases, of number of winners at that phase times number of msgs originated by that winner: The O(log n) algorithm is more complicated than the O(n^2) algorithm but uses fewer messages in worst case. Lower bound for LE AlgorithmCan we do better than O(nlogn) ? Theorem: Any leader election algorithm for asynchronous rings whose size is not known a priori has Ω(nlog n) msg complexity (holds also for undirectional rings). The two algorithms above are comparison-based algorithms, i.e. they use the identifiers only for comparison (&lt;, &gt;, =) In synchronous networks, O(n) msg complexity can be achieved if general arithmatic operations are permitted (non-comparison based) and if time complexity is unbounded. References[1] Attiya, Hagit, and Jennifer Welch. Distributed computing: fundamentals, simulations, and advanced topics. Vol. 19. John Wiley &amp; Sons, 2004.[2] 分布式算法（黄宇）课程主页[3] Distributed System]]></content>
      <categories>
        <category>Introduction to Distributed Algorithms</category>
      </categories>
      <tags>
        <tag>分布式算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L02-Basic Message Passing Algorithms]]></title>
    <url>%2F2017%2F10%2F05%2FL02-Basic-Message-Passing-Algorithms%2F</url>
    <content type="text"><![CDATA[Broadcast / convergecast on a spanning tree Async / sync flooding to construct a spanning tree distributed DFS with/without a specific root Broadcast over a rooted spanning tree Broadcast is used to send the information to all. Suppose processors already have information about a rooted spanning tree of the communication topology tree: connected graph with no cycles spanning tree: contains all processors rooted: there is a unique root node Implemented via parent and children local varialbes at each processor. indicate which incident channnels lead to parent and children in the rooted spanning tree. Spanning Tree: A tree is a connected undirected graph with no cycles. It is a spanning tree of a graph G if it spans G (that is, it includes every vertex of G) and is a subgraph of G (every edge in the tree belongs to G). A spanning tree of a connected graph G can also be defined as a maximal set of edges of G that contains no cycle, or as a minimal set of edges that connect all vertices. Complexity analysis: Synchronous model Time complexity: time is depth d of the spanning tree. (at most n-1 when chain) MSG complexity: number of messages is n-1, since one message is sent over each spanning tree edges. Aysnchronous model Same as synchronous model. Convergecast (from leaves to the root) Convergecast is used to collect the information. Again, suppose a rooted spanning tree has already been computed by the processors parent and children variables at each processor Do the opposite of broadcast leaves send msgs to their parents. non-leaves wait to get msgs from each child, then send combined (aggregate) info to parent. Finding a Spanning Tree Given a Root by Flooding Flooding): Flooding is a simple computer network routing algorithm in which every incoming packet is sent through every outgoing link except the one it arrived on. root send M to all its neighbours when non-root first gets M, set the sender as its parent send “parent” msg to sender send M to all other neighbours (if no other neighours, then terminate) when get M otherwise, send “reject” to sender. use “parent” and “reject” msgs to set children varialbes and know when to terminate (after hearing from all neighbours) Execution of spanning tree algorithm In the synchronous model: always gives breadth-first search (BFS) tree. Asynchronous: not necessarily BFS tree. Both models achieves O(m) messages complexity and O(diam) time complexity. Diameter D of a network is defined as the longest path of the shortest paths between any two nodes. Distributed DFS with a Specified Root Basic rationale: sequential execution over a distributed system (of multiple processors) Distributed DFS without a Specified Root Assume the processors have unique identifiers (otherwise impossible!) Idea: Each processor starts running a copy of the DFS spanning tree algorithm, with itself as root tag each msg with initiator’s id to differentiate when copies “collide”, copy with larger id wins. Message complexity: O(n*m) Time complexity: O(m) (m: edges in graph) References[1] Attiya, Hagit, and Jennifer Welch. Distributed computing: fundamentals, simulations, and advanced topics. Vol. 19. John Wiley &amp; Sons, 2004.[2] 分布式算法（黄宇）课程主页[3] Distributed System]]></content>
      <categories>
        <category>Introduction to Distributed Algorithms</category>
      </categories>
      <tags>
        <tag>分布式算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L01-Model of Computation]]></title>
    <url>%2F2017%2F09%2F27%2FL01-Model-of-Computation%2F</url>
    <content type="text"><![CDATA[Async/sync system Random access machine model Message passing model Shared memory model Essential Issues of Alogrithm Model of computation Algorithm design Algorithm analysis Asynchronous &amp; Synchronous System Asynchronous System. A system is said to be asynchronous if there is no fixed upper bound on how long it takes for a message to be delivered (message delays) or how much time elapses between consectutive steps of a processor (processor step time) [1]. Synchronous System. In the synchronous model processors execute in lockstep: The execution is partitioned into rounds, and in each round, every processor can send a message to each neighbour, the messages are delivered, and every processor computes based on the messages just received. (This model is convenient for designing algorithms) [1] Why asynchronous systems? Sometimes the upper bounds are quite large, are infrequently reached and change over time. It is often desiable to design an algorithm that is independent of any particular timing parameters, namely an asynchronous algorithm Instead of design an algorithms that depends on the bounds Random Access Machine (RAM) ModelThe goal of working with a model computer instead of a real computer is that we want to have a machine, which is as easy as possible, but still let us capture the main aspects of a real computer. This model of computation is an abstraction that allows us to compare algorithms on the basis of performance. Simplifications for RAM model: Simple operations take only 1 time step; Loops and subroutines are not simple operations; We assume we have as much memory as we need (infinite storage); Memory access is considered to be free in terms of time (or one time step?); A unit of memory cannot hold an arbitrarily large number. The RAM model takes no notice of whether an item is in cache or on the disk, which simplifies the analysis. It is an excellent model for understanding how an algorithm will perform on a real computer. It strikes a fine balance by capturing the essential behavior of computers while being simple to work with. We use the RAM model because it is useful in practice. Relationship between the Turing Machine and RAM ModelsA random-access machine with unbounded indirection is equivalent to a Turing machine. Informally speaking, both machines have the same computational capabilities. (wikipedia | Equivalance of RAM and Turing Machines) Message Passing ModelThe architecture is used to communicate data among a set of processors without the need for a global memory. Each processor has its own local memory and communicates with other Processors using message. Data exchanged among processors cannot be shared; it is rather copied (using send/receive messages). An important advantage of this form of data exchange is the elimination of the need for synchronization constructs, such as semaphores, which results in performance improvement. Shared Memory ModelBoth SMP and DSM are shared address space platforms. Symmetric Multiprocessors (SMP)Processors all connected to a large shared memory. Examples are processors connected by crossbar, or multicore chips. It is symmetric because the access time from any of the CPUs to memory is the same. Key characteristics is uniform memory acess (UMA).Caches are a problem: need to be kept coherrent = when one CPU changes a value in memory, then all other CPUs will get the same value when they access it. All caches will show a coherent value. Distributed Shared Memory (DSM)DSM is basically an abstraction that integrates the local memory of different machine into a single logical entity shared by cooperating processes. The distributed shared memory implements the shared memory model in distributed systems, which have no physical shared memory. (shared memory exists only virtually, similar concepts to virtual memory) The shared memory model provides a virtual address space shared between all nodes The overcome the high cost of communication in distributed systems, DSM systems move data to the location of access. How? Data moves between main memory and secondary memory (within a node) and between main memories of different nodes. Each data object is owned by a node Initial owner is the node that created object Ownership can change as object moves from node to node When a process accesses data in the shared address space, the mapping manager maps shared memory address to physical memory (local or remote). Shared Memory v.s. Message Passing Messsge Passing Shared Memory who does communication Programmer Automatic Data distribution Manual Automatic HW support Simple Extensive (automatically figures out when to send data, to whom and where to cache in, etc.) Programming Correctness Difficult Less Difficult Performance Difficult (noce you get correctness, performance is not far away) Very Difficult References[1] Attiya, Hagit, and Jennifer Welch. Distributed computing: fundamentals, simulations, and advanced topics. Vol. 19. John Wiley &amp; Sons, 2004.[2] 分布式系统(Distributed System)资料[3] Shared Memory, NYU Computer Science[4] 分布式算法（黄宇）课程主页[5] Message Passing Vs Shared Memory - Georgia Tech - HPCA: Part 5]]></content>
      <categories>
        <category>Introduction to Distributed Algorithms</category>
      </categories>
      <tags>
        <tag>分布式算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[htop - interactive process viewer]]></title>
    <url>%2F2017%2F06%2F18%2Fhtop%2F</url>
    <content type="text"><![CDATA[htop命令简介htop是Linux系统下的一个交互式、实时进程监控应用程序，top的高级版。 优点： 可以横向或纵向滚动浏览进程列表，以便查看所有的进程和完整的命令行 支持鼠标操作 杀进程时不需要输入进程号(快捷键: F9) htop vs top It is similar to top, but allows you to scroll vertically and horizon- tally, so you can see all the processes running on the system, along with their full command lines, as well as viewing them as a process tree, selecting multiple processes and acting on them all at once.Tasks related to processes (killing, renicing) can be done without entering their PIDs. (摘自htop手册:) 安装For Mac OS: 1$ brew install htop For Ubuntu 14.04 LTS: 1$ sudo apt-get install htop 查看命令手册： 1$ man htop 启动htop： 1$ htop 将得到如下类似的一个实时进程监控窗口： 帮助：按F1进入使用帮助。 退出：按下q键退出htop面板。 监控面板介绍系统CPU使用率 图中上半部分中的数字1到8表示系统中CPU/Core的数量，而紧邻数字的右侧进度条则相应地表示了对应CPU/Core的实时负载。进度条中不同颜色具有不同的含义(以下为默认配置，具体操作时可以在F2-&gt;Setup-&gt;Colors列表中选择不同的主题)： CPU使用率栏: [低优先级进程/用户进程/内核进程 &ensp; &ensp; &ensp; &ensp; &ensp; &ensp; &ensp; &ensp; &ensp; &ensp; used%] 系统内存使用率 Memory使用率栏：[已使用/buffers/cache &ensp; &ensp; &ensp; &ensp; &ensp; &ensp; &ensp; &ensp; &ensp; &ensp; used/total] 系统平均负载 负载(Load)): 运行队列(run-queue)的长度：L = 等待进程的数目 + 运行进程的数目 平均负载(load average) 在一段时间内CPU正在处理以及等待CPU处理的进程数之和的统计信息，也就是CPU使用队列的长度的统计信息 “最大负载=核心数”法则 在多核处理中，负载不应该高于处理器核心的总数量。source 上图中Load average之后的3个数字显示的是系统在1分钟，5分钟，15分钟之内的平均负载值。(注： uptime命令可以直接查看load average) 进程详细实时信息列表 每一列依次表示： PID 进程ID USER 进程的所有者 PRI 进程优先级。数字越小，优先级越高。 NI 进程的nice值（负值表示高优先级，正值表示低优先级） VIRT 进程使用的虚拟内存 RES 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA S 进程状态 R (Running) &ensp; &ensp; 可执行状态（运行/就绪) D (uninterruptedly sleeping) &ensp; &ensp; 不可中断的睡眠状态.通常是在等待IO，比如磁盘IO，网络IO，其他外设IO (该状态不接收外来的任何信号，因此无法用kill杀掉D状态的进程) S (sleeping) &ensp; &ensp; 可中断的睡眠状态(因为等待某某事件的发生(比如等待socket连接、等待信号量),而被挂起) T (traced) &ensp; &ensp; 暂停状态或跟踪状态 (例如在gdb中对被跟踪的进程下一个断点，进程在断点处停下来的时候就处于task_traced状态) Z (zombied) &ensp; &ensp; 退出状态，进程成为僵尸进程(已经结束了的进程，但是没有从进程表中删除) CPU% 进程的CPU时间片利用率 MEM% 进程的物理内存利用率 TIME+ 进程使用的处理器时间总计 Command 启动该进程的完整命令行 最后一行是F1~F10的功能菜单和对应的字母快捷键。 Read MoreUnderstanding the Load Average on Linux and Other Unix-like SystemsUnderstanding Linux CPU Load - when should you be worried?Understanding and using htop to monitor system resourcesLinux Performance Tools]]></content>
      <categories>
        <category>Linux性能分析工具</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Linux性能分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法基础]]></title>
    <url>%2F2017%2F06%2F15%2F%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[function doDecrypt(pwd, onError) { console.log("in doDecrypt"); var txt = document.getElementById("enc_content").innerHTML; var plantext; try { var bytes = CryptoJS.AES.decrypt(txt, pwd); plaintext = bytes.toString(CryptoJS.enc.Utf8); } catch(err) { if(onError) {onError(err);} return; } document.getElementById("enc_content").innerHTML = plaintext; document.getElementById("enc_content").style.display = "block"; document.getElementById("enc_passwd").style.display = "none"; } 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 var onError = function(error) { document.getElementById("enc_error").innerHTML = "password error!" }; function decrypt() { var passwd = document.getElementById("enc_pwd_input").value; console.log(passwd); doDecrypt(passwd, onError); }]]></content>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【闲情】记 第二届南大上海校友会新年羽毛球嘉年华]]></title>
    <url>%2F2017%2F01%2F07%2F%E9%97%B2%E6%83%85-%E8%AE%B0%E7%AC%AC%E4%BA%8C%E5%B1%8A%E5%8D%97%E5%A4%A7%E4%B8%8A%E6%B5%B7%E6%A0%A1%E5%8F%8B%E4%BC%9A%E6%96%B0%E5%B9%B4%E7%BE%BD%E6%AF%9B%E7%90%83%E5%98%89%E5%B9%B4%E5%8D%8E%2F</url>
    <content type="text"><![CDATA[Abstract 时间： 2017年1月7日 地点：上海财大（武川路校区）羽毛球馆（旧馆） 事件：第二届南大上海校友会新年羽毛球嘉年华活动 角色：第二次参加这个活动，本次担任 杨浦队 队长 ^_^ 赛制：趣味赛 + 团体赛 赛果：杨浦队在8支参赛队中勇夺 第4名 (历史最好成绩)！ 趣味赛: 单手颠球，穿越障碍 三人隔网，你发我接（限时） 团体赛(8支) 徐汇、闵松、沿江、张江、花木、杨浦、长静黄联队、市北联队 赛后聚餐地点：上海财大豪生酒店 赛前宣传海报(感谢@陈雨致 学姐的精心设计)： 赛前集体合影(每队一列，列首为该队队长): 高质量的比赛场馆(感谢@王绍立学长大力帮助联系和提供场地！): 比赛中： 赛果: 冠军：闵松队(根据约定，本次比赛冠军将负责主办明年的第三届比赛^_-) 亚军：沿江队 季军：张江队 殿军：杨浦队 赛后各队队长、裁判、志愿者们合影: 大杨浦队赛后合影留念 赛后聚餐: 广告时间感谢炜觉资本对本次活动的大力赞助！ 明年再来，闵松主办！]]></content>
      <categories>
        <category>闲情</category>
      </categories>
      <tags>
        <tag>闲情</tag>
        <tag>羽毛球</tag>
        <tag>相册</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CaffeNet vs CaffeOnSpark]]></title>
    <url>%2F2016%2F05%2F12%2FCaffeNet-vs-CaffeOnSpark%2F</url>
    <content type="text"><![CDATA[SparkNetSparkNet 是由amplab开发的基于Spark的分布式深度神经网络架构，2015年11月份在github上开源. 该系统开发的动机在于当下流行的批处理计算框架（MapReduce、Spark等）都不是设计用来支持已有的分布式深度学习系统下的异步的、通行密集型的任务。SparkNet将已有的数据处理框架（Spark)和流行的、高效的深度学习框架Caffe整合到一起，同时提供了Spark RDDs和Caffe的访问接口；并通过改进的并行化SGD算法来降低节点间的通信开销[1]. SparkNet的架构示例如图1所示[1]。 图1是由5个节点组成的EC2集群。其中Master节点负责向其余4各worker节点分发任务；每个worker在本地单独使用基于GPU的caffe来进行训练；本地训练结束后各个worker将参数回传给Master;Master收集各个worker的参数后做全局的处理（例如average）再broadcast给各个worker以进行下一轮的迭代。 CaffeOnSparkCaffeOnSpark 由Yahoo开发，并于今年2月份开源在github上。该系统旨在将深度学习步骤(训练、测试等)无缝地嵌入到Spark应用中，使得直接在存储数据的(强化的)Hadoop集群上进行深度学习成为可能，从而避免了数据在Hadoop集群和深度学习集群之间不必要的传输. 作为Spark的深度学习包，CaffeOnSpark填补了Spark MLlib在DL能力上的不足[3]。可以这样说，CaffeOnSpark集合了Caffe和Spark两者之长，并应用到大规模深度学习上，使得我们能像操作其他Spark应用一样操作深度学习任务。 CaffeOnSpark的系统架构图如图2[6] 所示： &ensp;&ensp; &ensp; &ensp; &ensp; &ensp; &ensp;&ensp; &ensp; &ensp; &ensp; &ensp; &ensp;图2. CaffeOnSpark System Architecture 从图2中我们看到CaffeOnSpark在参数的同步/管理操作上和SparkNet是完全不同的：CaffeOnSpark executors之间通过MPI_ALLREDUCE接口通信（通过底层RDMA/Infiniband 或者TCP/Ethenet来保证GPU间高速的数据传输,10x于CPU).在这种通信模式下，各个节点/Caffe引擎之间是peer-to-peer的模式；而SparkNet的设计中依然保留了Spark的主/从模式。 小结CaffeOnSpark和SparkNet的相同之处在于两者都使得Spark集群下基于Caffe的深度学习成为可能。 而两者最显著的不同在于两者在系统框架设计中对于参数同步所采用的不同方案。 参考资料[1]. Moritz,Philipp, et al. “SparkNet: Training Deep Networks in Spark.” arXiv preprint arXiv:1511.06051 (2015).[2]. Large Scale Distributed Deep Learning on Hadoop Clusters, PPT[3]. GitHub - amplab/SparkNet, https://github.com/amplab/SparkNet[4]. GitHub - yahoo/CaffeOnSpark, https://github.com/yahoo/CaffeOnSpark[5]. CaffeOnSpark Google Group, https://groups.google.com/forum/#!forum/sparknet-users[6]. Large Scale Distributed Deep Learning on Hadoop Clusters]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习框架</tag>
      </tags>
  </entry>
</search>